{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotPoseDataset(Dataset):\n",
    "    def __init__(self, pairs, transform=None, HEATMAP_SIZE=(128, 128), sigma=5.0):\n",
    "        self.pairs = pairs\n",
    "        self.transform = transform\n",
    "        self.heatmap_size = HEATMAP_SIZE\n",
    "        self.sigma = sigma\n",
    "\n",
    "        print(\"Loading and preprocessing metadata...\")\n",
    "        # --- 1. ArUco 데이터 로드 ---\n",
    "        self.aruco_lookup = {}\n",
    "        # pose1 데이터 로드\n",
    "        pose1_aruco_path = '../dataset/franka_research3/pose1_aruco_pose_summary.json'\n",
    "        with open(pose1_aruco_path, 'r') as f:\n",
    "            for item in json.load(f):\n",
    "                self.aruco_lookup[f\"pose1_{item['view']}_{item['cam']}\"] = item\n",
    "        # pose2 데이터 로드\n",
    "        pose2_aruco_path = '../dataset/franka_research3/pose2_aruco_pose_summary.json'\n",
    "        with open(pose2_aruco_path, 'r') as f:\n",
    "            for item in json.load(f):\n",
    "                self.aruco_lookup[f\"pose2_{item['view']}_{item['cam']}\"] = item\n",
    "        \n",
    "        # --- 2. Calibration 데이터 로드 ---\n",
    "        self.calib_lookup = {}\n",
    "        calib_dir = \"../dataset/franka_research3/Calib_cam_from_conf\"\n",
    "        for calib_path in glob.glob(os.path.join(calib_dir, \"*.json\")):\n",
    "            filename = os.path.basename(calib_path).replace(\"_calib.json\", \"\")\n",
    "            with open(calib_path, 'r') as f:\n",
    "                self.calib_lookup[filename] = json.load(f)\n",
    "        \n",
    "        # ▼▼▼ [수정] serial_to_view 딕셔너리를 __init__으로 이동 (성능 향상) ▼▼▼\n",
    "        self.serial_to_view = {\n",
    "            '41182735': \"view1\", '49429257': \"view2\",\n",
    "            '44377151': \"view3\", '49045152': \"view4\"\n",
    "        }\n",
    "                \n",
    "        print(f\"✅ Metadata loaded. Found {len(self.aruco_lookup)} ArUco entries and {len(self.calib_lookup)} calibration files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ▼▼▼ [수정] 안정성을 위해 try-except 구문 추가 ▼▼▼\n",
    "        try:\n",
    "            pair = self.pairs[idx]\n",
    "            image_path = pair['image_path']\n",
    "            \n",
    "            # --- 1. 파일 경로 분석 ---\n",
    "            filename = os.path.basename(image_path)\n",
    "            parts = filename.split('_')\n",
    "            serial_str = parts[1]\n",
    "            selected_cam = parts[2] + \"cam\"\n",
    "            selected_view = self.serial_to_view[serial_str]\n",
    "\n",
    "            # --- 2. 이미지 및 메타데이터 로드 ---\n",
    "            calib_key = f\"{selected_view}_{serial_str}_{selected_cam}\"\n",
    "            calib = self.calib_lookup[calib_key]\n",
    "            camera_matrix = np.array(calib[\"camera_matrix\"], dtype=np.float32)\n",
    "            dist_coeffs = np.array(calib[\"distortion_coeffs\"], dtype=np.float32)\n",
    "            \n",
    "            if 'pose1' in image_path:\n",
    "                pose_name = 'pose1'\n",
    "            elif 'pose2' in image_path:\n",
    "                pose_name = 'pose2'\n",
    "            else:\n",
    "                raise ValueError(f\"Image path does not contain 'pose1' or 'pose2': {image_path}\")\n",
    "            aruco_key = f\"{pose_name}_{selected_view}_{selected_cam}\"\n",
    "            aruco_result = self.aruco_lookup[aruco_key]\n",
    "\n",
    "            # --- 3. 이미지 처리 ---\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_np = np.array(image)\n",
    "            undistorted_image_np = cv2.undistort(image_np, camera_matrix, dist_coeffs)\n",
    "            undistorted_image = Image.fromarray(undistorted_image_np)\n",
    "            image_tensor = self.transform(undistorted_image)\n",
    "\n",
    "            # --- 4. Joint 데이터 로드 및 3D 좌표 계산 ---\n",
    "            joint_angle_data = pair['joint_angles']\n",
    "            gt_angles = torch.tensor(joint_angle_data, dtype=torch.float32)\n",
    "            \n",
    "            # ▼▼▼ [수정] 올바른 함수 이름 사용 및 불필요한 인자 제거 ▼▼▼\n",
    "            joint_coords_3d = forward_kinematics(joint_angle_data)\n",
    "\n",
    "            # --- 5. 3D->2D 투영 (단위 변환 포함) ---\n",
    "            # ▼▼▼ [수정] rvec 단위를 Degree에서 Radian으로 변환하는 로직 추가 ▼▼▼\n",
    "            rvec_deg = np.array([\n",
    "                aruco_result.get('rvec_x_deg', aruco_result.get('rvec_x', 0)),\n",
    "                aruco_result.get('rvec_y_deg', aruco_result.get('rvec_y', 0)),\n",
    "                aruco_result.get('rvec_z_deg', aruco_result.get('rvec_z', 0))\n",
    "            ], dtype=np.float32)\n",
    "            rvec = np.deg2rad(rvec_deg) # 단위 변환\n",
    "            \n",
    "            tvec = np.array([\n",
    "                aruco_result.get('tvec_x', aruco_result.get('mean_x', 0)),\n",
    "                aruco_result.get('tvec_y', aruco_result.get('mean_y', 0)),\n",
    "                aruco_result.get('tvec_z', aruco_result.get('mean_z', 0))\n",
    "            ], dtype=np.float32).reshape(3, 1)\n",
    "\n",
    "            # 왜곡 보정된 이미지에 투영하므로 dist_coeffs는 0으로 설정\n",
    "            pixel_coords, _ = cv2.projectPoints(joint_coords_3d, rvec, tvec, camera_matrix, np.zeros_like(dist_coeffs))\n",
    "            pixel_coords = pixel_coords.reshape(-1, 2)\n",
    "            \n",
    "            # --- 6. 히트맵 생성 ---\n",
    "            num_joints = len(pixel_coords)\n",
    "            original_h, original_w, _ = undistorted_image_np.shape\n",
    "\n",
    "            scaled_keypoints = np.zeros_like(pixel_coords)\n",
    "            scaled_keypoints[:, 0] = pixel_coords[:, 0] * (self.heatmap_size[1] / original_w)\n",
    "            scaled_keypoints[:, 1] = pixel_coords[:, 1] * (self.heatmap_size[0] / original_h)\n",
    "\n",
    "            gt_heatmaps_np = np.zeros((num_joints, self.heatmap_size[0], self.heatmap_size[1]), dtype=np.float32)\n",
    "            for i in range(num_joints):\n",
    "                gt_heatmaps_np[i] = create_gt_heatmap(scaled_keypoints[i], self.heatmap_size, self.sigma)\n",
    "            gt_heatmaps = torch.from_numpy(gt_heatmaps_np)\n",
    "\n",
    "            return image_tensor, gt_heatmaps, gt_angles\n",
    "\n",
    "        except Exception as e:\n",
    "            # 오류 발생 시 해당 샘플을 건너뛰도록 None을 반환\n",
    "            print(f\"⚠️ Warning: Skipping sample {idx} due to error in '{image_path}': {e}\")\n",
    "            return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ANGLES = 7\n",
    "NUM_JOINTS = 8\n",
    "FEATURE_DIM = 768\n",
    "HEATMAP_SIZE = (128, 128)\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers.image_utils import load_image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoImageProcessor\n",
    "from torchvision import transforms\n",
    "\n",
    "MODEL_NAME ='facebook/dinov3-convnext-tiny-pretrain-lvd1689m'\n",
    "# facebook/dinov3-vitb16-pretrain-lvd1689m\n",
    "# facebook/dinov3-convnext-base-pretrain-lvd1689m\n",
    "\n",
    "\n",
    "# 1. DINOv3 모델에 맞는 이미지 프로세서 로드\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2. 프로세서에서 평균과 표준편차 값 추출\n",
    "dino3_mean = processor.image_mean\n",
    "dino3_std = processor.image_std\n",
    "\n",
    "# 3. 이 값들을 사용하여 Transform 파이프라인 재구성\n",
    "# 예시: 학습용 Transform\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(processor.size[\"shortest_edge\"]),\n",
    "    transforms.CenterCrop(processor.crop_size[\"height\"]),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.15, saturation=0.15, hue=0.05),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.1, 0.2), ratio=(0.3, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dino3_mean, std=dino3_std) # ✅ DINOv3 값으로 변경\n",
    "])\n",
    "\n",
    "# 예시: 검증용 Transform\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(processor.size[\"shortest_edge\"]),\n",
    "    transforms.CenterCrop(processor.crop_size[\"height\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dino3_mean, std=dino3_std) # ✅ DINOv3 값으로 변경\n",
    "])\n",
    "\n",
    "\n",
    "class DINOv3Backbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Hugging Face transformers 라이브러리를 사용하여 DINOv3 모델을 구성합니다.\n",
    "    사전에 정규화된 이미지 텐서 배치를 입력받아 패치 토큰을 반환합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=MODEL_NAME): # ViT-Base 모델을 기본값으로 사용\n",
    "        super().__init__()\n",
    "        # 사전 학습된 DINOv3 모델을 불러옵니다.\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        # ⚠️ 참고: 모델을 특정 장치(.to('cuda'))로 보내는 코드는\n",
    "        # 메인 학습 스크립트에서 한 번에 처리하는 것이 좋습니다.\n",
    "\n",
    "    def forward(self, image_tensor_batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_tensor_batch (torch.Tensor): (B, C, H, W) 형태의 정규화된 이미지 텐서\n",
    "        \"\"\"\n",
    "        # 그래디언트 계산을 비활성화합니다.\n",
    "        with torch.no_grad():\n",
    "            # Hugging Face 모델은 'pixel_values'라는 키워드 인자를 기대합니다.\n",
    "            outputs = self.model(pixel_values=image_tensor_batch)\n",
    "\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        \n",
    "        # 클래스 토큰(CLS)을 제외한 패치 토큰들만 반환합니다.\n",
    "        patch_tokens = last_hidden_state[:, 1:, :]\n",
    "        \n",
    "        return patch_tokens\n",
    "\n",
    "class JointAngleHead(nn.Module):\n",
    "    def __init__(self, input_dim=FEATURE_DIM, num_angles=NUM_ANGLES, num_queries=4, nhead=8, num_decoder_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. \"로봇 포즈에 대해 질문하는\" 학습 가능한 쿼리 토큰 생성\n",
    "        self.pose_queries = nn.Parameter(torch.randn(1, num_queries, input_dim))\n",
    "        \n",
    "        # 2. PyTorch의 표준 Transformer Decoder 레이어 사용\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=input_dim, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=input_dim * 4, # 일반적인 설정\n",
    "            dropout=0.1, \n",
    "            activation='gelu',\n",
    "            batch_first=True  # (batch, seq, feature) 입력을 위함\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        \n",
    "        # 3. 최종 각도 예측을 위한 MLP\n",
    "        # 디코더를 거친 모든 쿼리 토큰의 정보를 사용\n",
    "        self.angle_predictor = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim * num_queries),\n",
    "            nn.Linear(input_dim * num_queries, 512),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, num_angles)\n",
    "        )\n",
    "\n",
    "    def forward(self, fused_features):\n",
    "        # fused_features: DINOv2의 패치 토큰들 (B, Num_Patches, Dim)\n",
    "        # self.pose_queries: 학습 가능한 쿼리 (1, Num_Queries, Dim)\n",
    "        \n",
    "        # 배치 사이즈만큼 쿼리를 복제\n",
    "        b = fused_features.size(0)\n",
    "        queries = self.pose_queries.repeat(b, 1, 1)\n",
    "        \n",
    "        # Transformer Decoder 연산\n",
    "        # 쿼리(queries)가 이미지 특징(fused_features)에 어텐션을 수행하여\n",
    "        # 포즈와 관련된 정보로 자신의 값을 업데이트합니다.\n",
    "        attn_output = self.transformer_decoder(tgt=queries, memory=fused_features)\n",
    "        \n",
    "        # 업데이트된 쿼리 토큰들을 하나로 펼쳐서 MLP에 전달\n",
    "        output_flat = attn_output.flatten(start_dim=1)\n",
    "        \n",
    "        return self.angle_predictor(output_flat)\n",
    "\n",
    "class MultiViewFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Latent Query 기반의 Multi-view Fusion 모듈.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=FEATURE_DIM, num_heads=8, dropout=0.1, num_queries=16, num_layers=2):\n",
    "        super().__init__()\n",
    "        # 씬 전체의 정보를 요약할 학습 가능한 글로벌 쿼리\n",
    "        self.global_queries = nn.Parameter(torch.randn(1, num_queries, feature_dim))\n",
    "        \n",
    "        # Cross-Attention + Self-Attention으로 구성된 Transformer Decoder 레이어\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=feature_dim, nhead=num_heads, dim_feedforward=feature_dim * 4,\n",
    "            dropout=dropout, activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.fusion_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, view_features: list):\n",
    "        # 1. 모든 뷰의 토큰들을 시퀀스 차원에서 하나로 합침\n",
    "        all_view_tokens = torch.cat(view_features, dim=1)\n",
    "        b = all_view_tokens.size(0)\n",
    "        \n",
    "        # 2. 배치 사이즈만큼 글로벌 쿼리 복제\n",
    "        queries = self.global_queries.repeat(b, 1, 1)\n",
    "        \n",
    "        # 3. Decoder를 통해 쿼리가 모든 뷰의 정보를 요약하도록 함\n",
    "        # 쿼리가 Key/Value인 all_view_tokens에 Cross-Attention을 수행하고,\n",
    "        # 이후 쿼리들끼리 Self-Attention을 수행하며 정보를 정제함\n",
    "        fused_queries = self.fusion_decoder(tgt=queries, memory=all_view_tokens)\n",
    "        \n",
    "        return fused_queries\n",
    "\n",
    "class TokenFuser(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT의 패치 토큰(1D 시퀀스)을 CNN이 사용하기 좋은 2D 특징 맵으로 변환하고 정제합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.refine_blocks = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        # x: (B, D, H, W) 형태로 reshape된 토큰 맵\n",
    "        projected = self.projection(x)\n",
    "        refined = self.refine_blocks(projected)\n",
    "        residual = self.residual_conv(x)\n",
    "        return torch.nn.functional.gelu(refined + residual)\n",
    "\n",
    "class LightCNNStem(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 간단한 CNN 블록 구성\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False), # 해상도 1/2\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=False), # 해상도 1/4\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False), # 해상도 1/8\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: 원본 이미지 텐서 배치 (B, 3, H, W)\n",
    "        feat_4 = self.conv_block1(x)  # 1/4 스케일 특징\n",
    "        feat_8 = self.conv_block2(feat_4) # 1/8 스케일 특징\n",
    "        return feat_4, feat_8 # 다른 해상도의 특징들을 반환\n",
    "\n",
    "class FusedUpsampleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    업샘플링된 특징과 CNN 스템의 고해상도 특징(스킵 연결)을 융합하는 블록.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.refine_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_feature):\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # ✅ 해결책: skip_feature를 x의 크기에 강제로 맞춥니다.\n",
    "        # ----------------------------------------------------------------------\n",
    "        # 두 텐서의 높이와 너비가 다를 경우, skip_feature를 x의 크기로 리사이즈합니다.\n",
    "        if x.shape[-2:] != skip_feature.shape[-2:]:\n",
    "            skip_feature = F.interpolate(\n",
    "                skip_feature, \n",
    "                size=x.shape[-2:], # target H, W\n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "        # ----------------------------------------------------------------------\n",
    "        \n",
    "        # 이제 두 텐서의 크기가 같아졌으므로 안전하게 합칠 수 있습니다.\n",
    "        fused = torch.cat([x, skip_feature], dim=1)\n",
    "        return self.refine_conv(fused)\n",
    "    \n",
    "class UNetViTKeypointHead(nn.Module):\n",
    "    def __init__(self, input_dim=768, num_joints=7, heatmap_size=(128, 128)):\n",
    "        super().__init__()\n",
    "        self.heatmap_size = heatmap_size\n",
    "        self.token_fuser = TokenFuser(input_dim, 256)\n",
    "        self.decoder_block1 = FusedUpsampleBlock(in_channels=256, skip_channels=64, out_channels=128)\n",
    "        self.decoder_block2 = FusedUpsampleBlock(in_channels=128, skip_channels=32, out_channels=64)\n",
    "        self.final_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.heatmap_predictor = nn.Conv2d(64, num_joints, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, dino_features, cnn_features):\n",
    "        cnn_feat_4, cnn_feat_8 = cnn_features\n",
    "\n",
    "        # 1. DINOv3 토큰을 표준 ViT 패치 개수인 196개로 잘라내고 2D 맵으로 변환\n",
    "        num_patches_to_keep = 196\n",
    "        dino_features_sliced = dino_features[:, :num_patches_to_keep, :]\n",
    "        \n",
    "        b, n, d = dino_features_sliced.shape\n",
    "        h = w = int(n**0.5)\n",
    "        x = dino_features_sliced.permute(0, 2, 1).reshape(b, d, h, w)\n",
    "\n",
    "        x = self.token_fuser(x)\n",
    "\n",
    "        # 2. 디코더 업샘플링 & 융합\n",
    "        x = self.decoder_block1(x, cnn_feat_8)\n",
    "        x = self.decoder_block2(x, cnn_feat_4)\n",
    "        \n",
    "        # 3. 최종 해상도로 업샘플링 및 예측\n",
    "        x = self.final_upsample(x)\n",
    "        heatmaps = self.heatmap_predictor(x)\n",
    "        \n",
    "        return F.interpolate(heatmaps, size=self.heatmap_size, mode='bilinear', align_corners=False)\n",
    "    \n",
    "class DINOv3PoseEstimator(nn.Module):\n",
    "    \"\"\"\n",
    "    [전체 아키텍처 (Overall Architecture)]\n",
    "    Multi-view 이미지들을 입력받아, 각 뷰의 특징을 추출하고 융합하여\n",
    "    하나의 통합된 관절 각도(global pose)와 각 뷰에 대한 키포인트 히트맵(local keypoints)을 예측합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=MODEL_NAME, num_joints=NUM_JOINTS, num_angles=NUM_ANGLES, max_views=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. 백본: 고차원 의미 정보 추출\n",
    "        self.backbone = DINOv3Backbone(model_name)\n",
    "        feature_dim = self.backbone.model.config.hidden_size\n",
    "        \n",
    "        # ▼▼▼ [수정 1] 뷰 임베딩을 유연하게 처리하기 위한 변경 ▼▼▼\n",
    "        # 최대 처리 가능한 카메라(뷰) 개수를 기반으로 임베딩 레이어 생성\n",
    "        self.view_embeddings = nn.Embedding(max_views, feature_dim)\n",
    "        \n",
    "        # forward 시점에 뷰 이름/시리얼과 인덱스를 동적으로 매핑할 딕셔너리\n",
    "        self.view_to_idx = {} \n",
    "        self.next_view_idx = 0\n",
    "\n",
    "        # 3. CNN 스템: 저차원 공간 정보 추출\n",
    "        self.cnn_stem = LightCNNStem()\n",
    "        \n",
    "        # 4. 퓨전 모듈: 모든 뷰의 정보를 하나의 전역 요약 정보로 압축\n",
    "        self.fusion_module = MultiViewFusion(feature_dim=feature_dim)\n",
    "        \n",
    "        # 5. 헤드 (예측기)\n",
    "        self.angle_head = JointAngleHead(input_dim=feature_dim, num_angles=num_angles, num_queries=16)\n",
    "        self.keypoint_head = UNetViTKeypointHead(input_dim=feature_dim, num_joints=num_joints)\n",
    "        self.keypoint_enricher = nn.TransformerDecoderLayer(\n",
    "            d_model=feature_dim, nhead=8, dim_feedforward=feature_dim * 4,\n",
    "            dropout=0.1, activation='gelu', batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, multi_view_images: dict):\n",
    "        all_dino_features_with_embed = []\n",
    "        all_cnn_features = {}\n",
    "        view_keys_ordered = list(multi_view_images.keys())\n",
    "\n",
    "        # --- Step 1: 각 뷰에 대한 병렬 특징 추출 ---\n",
    "        for view_key in view_keys_ordered: # key는 'front' 또는 '41182735' 등이 될 수 있음\n",
    "            view_tensor = multi_view_images[view_key]\n",
    "            dino_features = self.backbone(view_tensor)\n",
    "            \n",
    "            # ▼▼▼ [수정 2] 동적 뷰 인덱싱 로직 ▼▼▼\n",
    "            # 이전에 보지 못한 뷰(카메라)라면, 새로운 인덱스를 할당합니다.\n",
    "            if view_key not in self.view_to_idx:\n",
    "                if self.next_view_idx >= self.view_embeddings.num_embeddings:\n",
    "                    raise ValueError(f\"Exceeded maximum number of views ({self.view_embeddings.num_embeddings}).\")\n",
    "                self.view_to_idx[view_key] = self.next_view_idx\n",
    "                self.next_view_idx += 1\n",
    "            \n",
    "            view_idx = self.view_to_idx[view_key]\n",
    "            \n",
    "            # 해당 인덱스의 임베딩 벡터를 DINO 특징에 더해줍니다.\n",
    "            embedding = self.view_embeddings(\n",
    "                torch.tensor([view_idx], device=dino_features.device)\n",
    "            ).unsqueeze(0)\n",
    "            all_dino_features_with_embed.append(dino_features + embedding)\n",
    "            \n",
    "            all_cnn_features[view_key] = self.cnn_stem(view_tensor)\n",
    "\n",
    "        # --- Step 2: Multi-view 정보 융합 ---\n",
    "        # Latent Query를 통해 모든 뷰의 DINO 특징을 'fused_queries'라는 전역 정보로 요약\n",
    "        fused_queries = self.fusion_module(all_dino_features_with_embed)\n",
    "        \n",
    "        # --- Step 3: 관절 각도 예측 ---\n",
    "        # 요약된 전역 정보로부터 직접 관절 각도를 예측\n",
    "        predicted_angles = self.angle_head(fused_queries)\n",
    "        \n",
    "        # --- Step 4: 키포인트 히트맵 예측 ---\n",
    "        predicted_heatmaps_dict = {}\n",
    "        for i, view_name in enumerate(view_names_ordered):\n",
    "            # 4a. Keypoint Enricher: i번째 뷰의 특징(tgt)이 전역 요약 정보(memory)를 참고하여 스스로를 보강\n",
    "            enriched_tokens = self.keypoint_enricher(\n",
    "                tgt=all_dino_features_with_embed[i], \n",
    "                memory=fused_queries\n",
    "            )\n",
    "            # 4b. 보강된 토큰과 해당 뷰의 CNN 공간 특징을 이용해 최종 히트맵 예측\n",
    "            heatmap = self.keypoint_head(enriched_tokens, all_cnn_features[view_name])\n",
    "            predicted_heatmaps_dict[view_name] = heatmap\n",
    "        \n",
    "        return predicted_heatmaps_dict, predicted_angles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
