"""
DREAM ë°ì´í„°ì…‹ìœ¼ë¡œ DINOv2 Pose Estimator ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.
torchrunì„ ì‚¬ìš©í•œ ë¶„ì‚° í•™ìŠµ(DDP)ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì‹¤í–‰ ì˜ˆì‹œ (GPU 3ê°œ ì‚¬ìš©):
torchrun --nproc_per_node=3 DREAM_Train.py
"""

# ==============================================================================
# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ==============================================================================
import os
import glob
import json
import numpy as np
import random
import wandb
import threading
import time

import cv2
import math
from scipy.spatial.transform import Rotation as R
from tqdm import tqdm

import timm
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR
import kornia.augmentation as K

from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
import pandas as pd

# DDP ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler


# ==============================================================================
# 1. ìƒìˆ˜ ë° ì „ì—­ ë³€ìˆ˜ ì •ì˜
# ==============================================================================
NUM_ANGLES = 7
NUM_JOINTS = 7
FEATURE_DIM = 768
HEATMAP_SIZE = (128, 128)
REQUIRED_KEYPOINTS = ['panda_link0', 'panda_link2', 'panda_link3', 'panda_link4', 'panda_link6', 'panda_link7', 'panda_hand']


# ==============================================================================
# 2. í´ë˜ìŠ¤ ë° í•¨ìˆ˜ ì •ì˜
# ==============================================================================

# í—¬í¼ í•¨ìˆ˜: ê°€ìš°ì‹œì•ˆ íˆíŠ¸ë§µ ìƒì„±
def create_gt_heatmap(keypoint_2d, HEATMAP_SIZE, sigma):
    # (ì´ì „ì— ì œê³µëœ create_gt_heatmap í•¨ìˆ˜ ì „ì²´ë¥¼ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”)
    H, W = HEATMAP_SIZE
    x, y = keypoint_2d
    xx, yy = np.meshgrid(np.arange(W), np.arange(H))
    dist_sq = (xx - x)**2 + (yy - y)**2
    exponent = dist_sq / (2 * sigma**2)
    heatmap = np.exp(-exponent)
    heatmap[heatmap < np.finfo(float).eps * heatmap.max()] = 0
    return heatmap

# ë°ì´í„°ì…‹ í´ë˜ìŠ¤
class RobotPoseDataset(Dataset):
    def __init__(self, pairs, transform=None, heatmap_size=(128, 128), sigma=3.0):
        self.pairs = pairs
        self.transform = transform
        self.heatmap_size = heatmap_size
        self.sigma = sigma
        self.calib_lookup = {}
        DATA_PATHS = [
            '../dataset/DREAM_real/panda-3cam_azure',
            '../dataset/DREAM_real/panda-3cam_kinect360',
            '../dataset/DREAM_real/panda-3cam_realsense',
            '../dataset/DREAM_real/panda-orb',
        ]
        for base_path in DATA_PATHS:
            calib_path = os.path.join(base_path, '_camera_settings.json')
            try:
                with open(calib_path, 'r') as f:
                    calib_data = json.load(f)
                intrinsics = calib_data['camera_settings'][0]['intrinsic_settings']
                fx, fy, cx, cy = intrinsics['fx'], intrinsics['fy'], intrinsics['cx'], intrinsics['cy']
                camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float32)
                distortion_coeffs = np.zeros(5, dtype=np.float32)
                self.calib_lookup[base_path] = {"camera_matrix": camera_matrix, "distortion_coeffs": distortion_coeffs}
            except Exception as e:
                pass
    def __len__(self):
        return len(self.pairs)
    def __getitem__(self, idx):
        pair = self.pairs[idx]
        image_path = pair['image_path']
        try:
            calib_data = None
            for base_path, calib in self.calib_lookup.items():
                if image_path.startswith(base_path):
                    calib_data = calib
                    break
            if calib_data is None: return None, None, None
            camera_matrix = calib_data["camera_matrix"]
            dist_coeffs = calib_data["distortion_coeffs"]
            image = Image.open(image_path).convert('RGB')
            image_np = np.array(image)
            undistorted_image_np = cv2.undistort(image_np, camera_matrix, dist_coeffs)
            original_h, original_w, _ = undistorted_image_np.shape
            undistorted_image = Image.fromarray(undistorted_image_np)
            image_tensor = self.transform(undistorted_image) if self.transform else transforms.ToTensor()(undistorted_image)
            gt_angles = torch.tensor(pair['joint_angles'], dtype=torch.float32)
            keypoints_2d = pair['keypoints_2d']
            num_keypoints = len(REQUIRED_KEYPOINTS)
            gt_heatmaps_np = np.zeros((num_keypoints, self.heatmap_size[0], self.heatmap_size[1]), dtype=np.float32)
            for i, name in enumerate(REQUIRED_KEYPOINTS):
                x, y = keypoints_2d[name]
                scaled_x = x * (self.heatmap_size[1] / original_w)
                scaled_y = y * (self.heatmap_size[0] / original_h)
                gt_heatmaps_np[i] = create_gt_heatmap((scaled_x, scaled_y), self.heatmap_size, self.sigma)
            gt_heatmaps = torch.from_numpy(gt_heatmaps_np)
            return image_tensor, gt_heatmaps, gt_angles
        except Exception as e:
            return None, None, None

        

class DINOv2Backbone(nn.Module):
    def __init__(self, model_name='vit_base_patch14_dinov2.lvd142m'):
        super().__init__()
        self.model = timm.create_model(model_name, pretrained=True)

    def forward(self, image_tensor_batch): # ì…ë ¥ì´ í…ì„œ ë°°ì¹˜ë¡œ ë³€ê²½
        with torch.no_grad():
            features = self.model.forward_features(image_tensor_batch)
            patch_tokens = features[:, 1:, :]
        return patch_tokens

class JointAngleHead(nn.Module):
    def __init__(self, input_dim=FEATURE_DIM, num_angles=NUM_ANGLES, num_queries=4, nhead=8, num_decoder_layers=2):
        """
        ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ íŠ¹ì§•ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í—¤ë“œ.

        Args:
            input_dim (int): DINOv2 íŠ¹ì§• ë²¡í„°ì˜ ì°¨ì›.
            num_angles (int): ì˜ˆì¸¡í•  ê´€ì ˆ ê°ë„ì˜ ìˆ˜.
            num_queries (int): í¬ì¦ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ì˜ ìˆ˜.
            nhead (int): Multi-head Attentionì˜ í—¤ë“œ ìˆ˜.
            num_decoder_layers (int): Transformer Decoder ë ˆì´ì–´ì˜ ìˆ˜.
        """
        super().__init__()
        
        # 1. "ë¡œë´‡ í¬ì¦ˆì— ëŒ€í•´ ì§ˆë¬¸í•˜ëŠ”" í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ í† í° ìƒì„±
        self.pose_queries = nn.Parameter(torch.randn(1, num_queries, input_dim))
        
        # 2. PyTorchì˜ í‘œì¤€ Transformer Decoder ë ˆì´ì–´ ì‚¬ìš©
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=input_dim, 
            nhead=nhead, 
            dim_feedforward=input_dim * 4, # ì¼ë°˜ì ì¸ ì„¤ì •
            dropout=0.1, 
            activation='gelu',
            batch_first=True  # (batch, seq, feature) ì…ë ¥ì„ ìœ„í•¨
        )
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)
        
        # 3. ìµœì¢… ê°ë„ ì˜ˆì¸¡ì„ ìœ„í•œ MLP
        # ë””ì½”ë”ë¥¼ ê±°ì¹œ ëª¨ë“  ì¿¼ë¦¬ í† í°ì˜ ì •ë³´ë¥¼ ì‚¬ìš©
        self.angle_predictor = nn.Sequential(
            nn.LayerNorm(input_dim * num_queries),
            nn.Linear(input_dim * num_queries, 512),
            nn.GELU(),
            nn.LayerNorm(512),
            nn.Linear(512, 256),
            nn.GELU(),
            nn.LayerNorm(256),
            nn.Linear(256, num_angles)
        )

    def forward(self, fused_features):
        # fused_features: DINOv2ì˜ íŒ¨ì¹˜ í† í°ë“¤ (B, Num_Patches, Dim)
        # self.pose_queries: í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ (1, Num_Queries, Dim)
        
        # ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ ì¿¼ë¦¬ë¥¼ ë³µì œ
        b = fused_features.size(0)
        queries = self.pose_queries.repeat(b, 1, 1)
        
        # Transformer Decoder ì—°ì‚°
        # ì¿¼ë¦¬(queries)ê°€ ì´ë¯¸ì§€ íŠ¹ì§•(fused_features)ì— ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì—¬
        # í¬ì¦ˆì™€ ê´€ë ¨ëœ ì •ë³´ë¡œ ìì‹ ì˜ ê°’ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        attn_output = self.transformer_decoder(tgt=queries, memory=fused_features)
        
        # ì—…ë°ì´íŠ¸ëœ ì¿¼ë¦¬ í† í°ë“¤ì„ í•˜ë‚˜ë¡œ í¼ì³ì„œ MLPì— ì „ë‹¬
        output_flat = attn_output.flatten(start_dim=1)
        
        return self.angle_predictor(output_flat)

class TokenFuser(nn.Module):
    """
    ViTì˜ íŒ¨ì¹˜ í† í°(1D ì‹œí€€ìŠ¤)ì„ CNNì´ ì‚¬ìš©í•˜ê¸° ì¢‹ì€ 2D íŠ¹ì§• ë§µìœ¼ë¡œ ë³€í™˜í•˜ê³  ì •ì œí•©ë‹ˆë‹¤.
    """
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.projection = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.refine_blocks = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.GELU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
    def forward(self, x):
        # x: (B, D, H, W) í˜•íƒœë¡œ reshapeëœ í† í° ë§µ
        projected = self.projection(x)
        refined = self.refine_blocks(projected)
        residual = self.residual_conv(x)
        return torch.nn.functional.gelu(refined + residual)

class LightCNNStem(nn.Module):
    """
    UNetì˜ ì¸ì½”ë”ì²˜ëŸ¼ ê³ í•´ìƒë„ì˜ ê³µê°„ì  íŠ¹ì§•(shallow features)ì„ 
    ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ë¡œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ê²½ëŸ‰ CNN.
    """
    def __init__(self):
        super().__init__()
        # ê°„ë‹¨í•œ CNN ë¸”ë¡ êµ¬ì„±
        self.conv_block1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False), # í•´ìƒë„ 1/2
            nn.BatchNorm2d(16),
            nn.GELU(),
            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=False), # í•´ìƒë„ 1/4
            nn.BatchNorm2d(32),
            nn.GELU()
        )
        self.conv_block2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False), # í•´ìƒë„ 1/8
            nn.BatchNorm2d(64),
            nn.GELU()
        )
        
    def forward(self, x):
        # x: ì›ë³¸ ì´ë¯¸ì§€ í…ì„œ ë°°ì¹˜ (B, 3, H, W)
        feat_4 = self.conv_block1(x)  # 1/4 ìŠ¤ì¼€ì¼ íŠ¹ì§•
        feat_8 = self.conv_block2(feat_4) # 1/8 ìŠ¤ì¼€ì¼ íŠ¹ì§•
        return feat_4, feat_8 # ë‹¤ë¥¸ í•´ìƒë„ì˜ íŠ¹ì§•ë“¤ì„ ë°˜í™˜

class FusedUpsampleBlock(nn.Module):
    """
    ì—…ìƒ˜í”Œë§ëœ íŠ¹ì§•ê³¼ CNN ìŠ¤í…œì˜ ê³ í•´ìƒë„ íŠ¹ì§•(ìŠ¤í‚µ ì—°ê²°)ì„ ìœµí•©í•˜ëŠ” ë¸”ë¡.
    """
    def __init__(self, in_channels, skip_channels, out_channels):
        super().__init__()
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.refine_conv = nn.Sequential(
            nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.GELU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.GELU()
        )

    def forward(self, x, skip_feature):
        x = self.upsample(x)
        
        # âœ… í•´ê²°ì±…: skip_featureë¥¼ xì˜ í¬ê¸°ì— ê°•ì œë¡œ ë§ì¶¥ë‹ˆë‹¤.
        # ----------------------------------------------------------------------
        # ë‘ í…ì„œì˜ ë†’ì´ì™€ ë„ˆë¹„ê°€ ë‹¤ë¥¼ ê²½ìš°, skip_featureë¥¼ xì˜ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•©ë‹ˆë‹¤.
        if x.shape[-2:] != skip_feature.shape[-2:]:
            skip_feature = F.interpolate(
                skip_feature, 
                size=x.shape[-2:], # target H, W
                mode='bilinear', 
                align_corners=False
            )
        # ----------------------------------------------------------------------
        
        # ì´ì œ ë‘ í…ì„œì˜ í¬ê¸°ê°€ ê°™ì•„ì¡Œìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ í•©ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        fused = torch.cat([x, skip_feature], dim=1)
        return self.refine_conv(fused)
    
class UNetViTKeypointHead(nn.Module):
    def __init__(self, input_dim=768, num_joints=NUM_JOINTS, heatmap_size=(128, 128)):
        super().__init__()
        self.heatmap_size = heatmap_size
        self.token_fuser = TokenFuser(input_dim, 256)
        self.decoder_block1 = FusedUpsampleBlock(in_channels=256, skip_channels=64, out_channels=128)
        self.decoder_block2 = FusedUpsampleBlock(in_channels=128, skip_channels=32, out_channels=64)
        self.final_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.heatmap_predictor = nn.Conv2d(64, num_joints, kernel_size=3, padding=1)

    def forward(self, dino_features, cnn_features):
        cnn_feat_4, cnn_feat_8 = cnn_features

        # 1. DINOv3 í† í°ì„ í‘œì¤€ ViT íŒ¨ì¹˜ ê°œìˆ˜ì¸ 196ê°œë¡œ ì˜ë¼ë‚´ê³  2D ë§µìœ¼ë¡œ ë³€í™˜
        num_patches_to_keep = 196
        dino_features_sliced = dino_features[:, :num_patches_to_keep, :]
        
        b, n, d = dino_features_sliced.shape
        h = w = int(n**0.5)
        x = dino_features_sliced.permute(0, 2, 1).reshape(b, d, h, w)

        x = self.token_fuser(x)

        # 2. ë””ì½”ë” ì—…ìƒ˜í”Œë§ & ìœµí•©
        x = self.decoder_block1(x, cnn_feat_8)
        x = self.decoder_block2(x, cnn_feat_4)
        
        # 3. ìµœì¢… í•´ìƒë„ë¡œ ì—…ìƒ˜í”Œë§ ë° ì˜ˆì¸¡
        x = self.final_upsample(x)
        heatmaps = self.heatmap_predictor(x)
        
        return F.interpolate(heatmaps, size=self.heatmap_size, mode='bilinear', align_corners=False)
    
class DINOv2PoseEstimator(nn.Module):
    def __init__(self, model_name='vit_base_patch14_dinov2.lvd142m', num_joints=NUM_JOINTS, num_angles=NUM_ANGLES):
        super().__init__()
        self.backbone = DINOv2Backbone(model_name)
        feature_dim = self.backbone.model.embed_dim # timm ëª¨ë¸ì€ embed_dim ì‚¬ìš©
        
        self.cnn_stem = LightCNNStem()
        # í—¤ë“œ ìƒì„± ì‹œ ì¸ìë¥¼ ì „ë‹¬ë°›ì•„ ì‚¬ìš©
        self.keypoint_head = UNetViTKeypointHead(input_dim=feature_dim, num_joints=num_joints)
        self.angle_head = JointAngleHead(input_dim=feature_dim, num_angles=num_angles)

    def forward(self, image_tensor_batch):
        # 1. ë‘ ê²½ë¡œë¡œ ë³‘ë ¬ì ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
        dino_features = self.backbone(image_tensor_batch)      # ì˜ë¯¸ ì •ë³´
        cnn_stem_features = self.cnn_stem(image_tensor_batch) # ê³µê°„ ì •ë³´
        
        # 2. ê° í—¤ë“œì— í•„ìš”í•œ íŠ¹ì§• ì „ë‹¬
        predicted_heatmaps = self.keypoint_head(dino_features, cnn_stem_features)
        predicted_angles = self.angle_head(dino_features)
        
        return predicted_heatmaps, predicted_angles

# ==============================================================================
# 2. í•™ìŠµ/ê²€ì¦/ì‹œê°í™” í•¨ìˆ˜ ì •ì˜
# ==============================================================================
# ==============================================================================
# 2. ë°ì´í„°ì…‹ ì‹œê°í™” (Dataset Visualization)
# ==============================================================================

def visualize_dataset_sample(dataset, config, num_samples=3):
    """ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œì„ ì‹œê°í™”í•˜ì—¬ GTê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    print("\n--- Visualizing Dataset Samples ---")
    
    # ì—­ì •ê·œí™”(Un-normalization)ë¥¼ ìœ„í•œ ê°’
    mean = np.array(config['mean'])
    std = np.array(config['std'])

    for i in range(num_samples):
        # ëœë¤ ìƒ˜í”Œ ì„ íƒ
        idx = random.randint(0, len(dataset) - 1)
        image_tensor, gt_heatmaps, gt_angles = dataset[idx]
        
        # 1. ì´ë¯¸ì§€ í…ì„œë¥¼ ì‹œê°í™”ë¥¼ ìœ„í•œ Numpy ë°°ì—´ë¡œ ë³€í™˜
        img_np = image_tensor.numpy().transpose((1, 2, 0))
        img_np = std * img_np + mean # ì—­ì •ê·œí™”
        img_np = np.clip(img_np, 0, 1)

        # 2. GT íˆíŠ¸ë§µì„ í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œ ê²°í•©
        composite_heatmap = torch.sum(gt_heatmaps, dim=0).numpy()
        
        # 3. GT íˆíŠ¸ë§µì—ì„œ í‚¤í¬ì¸íŠ¸ ì¢Œí‘œ ì¶”ì¶œ
        keypoints = []
        h, w = gt_heatmaps.shape[1:]
        for j in range(gt_heatmaps.shape[0]):
            heatmap = gt_heatmaps[j]
            max_val_idx = torch.argmax(heatmap)
            y, x = np.unravel_index(max_val_idx.numpy(), (h, w))
            keypoints.append([x, y])
        keypoints = np.array(keypoints)

        # í‚¤í¬ì¸íŠ¸ ì¢Œí‘œë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§
        img_h, img_w, _ = img_np.shape
        scaled_keypoints = keypoints.copy().astype(float)
        scaled_keypoints[:, 0] *= (img_w / w)
        scaled_keypoints[:, 1] *= (img_h / h)
        
        heatmap_resized = cv2.resize(composite_heatmap, (img_w, img_h), interpolation=cv2.INTER_LINEAR)
        # 4. ì‹œê°í™”
        fig, axes = plt.subplots(1, 3, figsize=(9, 3))
        
        # ì›ë³¸ ì´ë¯¸ì§€
        axes[0].imshow(img_np)
        axes[0].set_title(f'Sample {idx+1}: Undistorted Image')
        axes[0].axis('off')
        
        # GT íˆíŠ¸ë§µ
        axes[1].imshow(img_np, alpha=0.6)
        axes[1].imshow(heatmap_resized, cmap='jet', alpha=0.4)
        axes[1].set_title('Ground Truth Heatmap Overlay')
        axes[1].axis('off')

        # GT í‚¤í¬ì¸íŠ¸
        axes[2].imshow(img_np)
        axes[2].scatter(scaled_keypoints[:, 0], scaled_keypoints[:, 1], c='lime', s=40, edgecolors='black', linewidth=1)
        axes[2].set_title('Ground Truth Keypoints')
        axes[2].axis('off')

        plt.suptitle(f"GT Angles: " + ", ".join([f"{a:.2f}" for a in gt_angles.numpy()]))
        plt.tight_layout()
        plt.show()

def visualize_predictions(model, dataset, device, config, epoch_num, num_samples=3):
    """
    Validation ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œì— ëŒ€í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ Ground Truthì™€ í•¨ê»˜ ì‹œê°í™”í•©ë‹ˆë‹¤.
    (1í–‰ 4ì—´ í”Œë¡¯ìœ¼ë¡œ ë³€ê²½)
    """
    print(f"\n--- Visualizing Predictions for Epoch {epoch_num} ---")
    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    
    mean = np.array(config['mean'])
    std = np.array(config['std'])

    for i in range(num_samples):
        idx = random.randint(0, len(dataset) - 1)
        image_tensor, gt_heatmaps, gt_angles = dataset[idx]
        
        # --- ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ ---
        with torch.no_grad():
            image_batch = image_tensor.unsqueeze(0).to(device)
            pred_heatmaps_batch, pred_angles_batch = model(image_batch)
            
            pred_heatmaps = pred_heatmaps_batch[0].cpu()
            pred_angles = pred_angles_batch[0].cpu()

        # --- ì‹œê°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ ---
        img_np = image_tensor.numpy().transpose((1, 2, 0))
        img_np = std * img_np + mean
        img_np = np.clip(img_np, 0, 1)
        img_h, img_w, _ = img_np.shape

        gt_composite_heatmap = torch.sum(gt_heatmaps, dim=0).numpy()
        gt_heatmap_resized = cv2.resize(gt_composite_heatmap, (img_w, img_h), interpolation=cv2.INTER_LINEAR)
        
        pred_composite_heatmap = torch.sum(pred_heatmaps, dim=0).numpy()
        pred_heatmap_resized = cv2.resize(pred_composite_heatmap, (img_w, img_h), interpolation=cv2.INTER_LINEAR)

        # GT í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ë° ìŠ¤ì¼€ì¼ë§
        gt_keypoints = []
        h, w = gt_heatmaps.shape[1:]
        for j in range(gt_heatmaps.shape[0]):
            y, x = np.unravel_index(torch.argmax(gt_heatmaps[j]).numpy(), (h, w))
            gt_keypoints.append([x * (img_w/w), y * (img_h/h)])
        gt_keypoints = np.array(gt_keypoints)
        
        # ì˜ˆì¸¡ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ë° ìŠ¤ì¼€ì¼ë§
        pred_keypoints = []
        for j in range(pred_heatmaps.shape[0]):
            y, x = np.unravel_index(torch.argmax(pred_heatmaps[j]).numpy(), (h, w))
            pred_keypoints.append([x * (img_w/w), y * (img_h/h)])
        pred_keypoints = np.array(pred_keypoints)

        # --- 1í–‰ 4ì—´ ì„œë¸Œí”Œë¡¯ìœ¼ë¡œ GTì™€ ì˜ˆì¸¡ ë¹„êµ ì‹œê°í™” ---
        fig, axes = plt.subplots(1, 4, figsize=(18, 5)) # âœ… figsizeë„ ì ì ˆí•˜ê²Œ ì¡°ì •

        # 1. GT íˆíŠ¸ë§µ ì˜¤ë²„ë ˆì´
        axes[0].imshow(img_np, alpha=0.7)
        axes[0].imshow(gt_heatmap_resized, cmap='jet', alpha=0.3)
        axes[0].set_title('GT Heatmap')
        axes[0].axis('off')
        
        # 2. ì˜ˆì¸¡ íˆíŠ¸ë§µ ì˜¤ë²„ë ˆì´
        axes[1].imshow(img_np, alpha=0.7)
        axes[1].imshow(pred_heatmap_resized, cmap='jet', alpha=0.3)
        axes[1].set_title('Pred Heatmap')
        axes[1].axis('off')

        # 3. GT í‚¤í¬ì¸íŠ¸
        axes[2].imshow(img_np)
        axes[2].scatter(gt_keypoints[:, 0], gt_keypoints[:, 1], c='lime', s=40, edgecolors='black', linewidth=1, label='GT')
        axes[2].set_title('GT Keypoints')
        axes[2].axis('off')
        
        # 4. ì˜ˆì¸¡ í‚¤í¬ì¸íŠ¸
        axes[3].imshow(img_np)
        axes[3].scatter(pred_keypoints[:, 0], pred_keypoints[:, 1], c='red', s=40, marker='x', linewidth=1, label='Pred')
        axes[3].set_title('Pred Keypoints')
        axes[3].axis('off')

        # GT ê°ë„ì™€ ì˜ˆì¸¡ ê°ë„ë¥¼ ì œëª©ì— í•¨ê»˜ í‘œì‹œ
        gt_str = "GT Angles: " + ", ".join([f"{a:.2f}" for a in gt_angles.numpy()])
        pred_str = "Pred Angles: " + ", ".join([f"{a:.2f}" for a in pred_angles.numpy()])
        plt.suptitle(f"Sample {idx+1} | Epoch {epoch_num}\n{gt_str}\n{pred_str}", fontsize=10)
        plt.tight_layout(rect=[0, 0.03, 1, 0.90]) # suptitleê³¼ ê²¹ì¹˜ì§€ ì•Šê²Œ ì¡°ì •
        # plt.show()
    return fig

def log_predictions_to_wandb(model, images, gt_heatmaps, gt_angles, device, config, title):
    """
    ì£¼ì–´ì§„ ë°ì´í„° ë°°ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì˜ˆì¸¡ì„ ì‹œê°í™”í•˜ê³  wandbì— ë¡œê¹…í•©ë‹ˆë‹¤.
    """
    model.eval() # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
    
    # ì—­ì •ê·œí™”ë¥¼ ìœ„í•œ ê°’
    mean = np.array(config['mean'])
    std = np.array(config['std'])
    
    log_images = []
    
    with torch.no_grad():
        # ì…ë ¥ëœ ì´ë¯¸ì§€ ë°°ì¹˜ ì „ì²´ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
        images_to_device = images.to(device)
        pred_heatmaps_batch, pred_angles_batch = model(images_to_device)

    # ë°°ì¹˜ ë‚´ ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ì‹œê°í™” ìë£Œ ìƒì„± (ìµœëŒ€ 5ê°œ)
    for i in range(min(images.shape[0], 5)):
        img_tensor = images[i]
        
        # í…ì„œë¥¼ ì‹œê°í™”ìš© Numpy ë°°ì—´ë¡œ ë³€í™˜ ë° ì—­ì •ê·œí™”
        img_np = img_tensor.cpu().numpy().transpose((1, 2, 0))
        img_np = std * img_np + mean
        img_np = np.clip(img_np * 255, 0, 255).astype(np.uint8)
        img_h, img_w, _ = img_np.shape
        
        # BGR ë³€í™˜ (OpenCVëŠ” BGR ìˆœì„œë¥¼ ì‚¬ìš©)
        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)

        # --- GT ì‹œê°í™” ---
        gt_hmap = gt_heatmaps[i]
        gt_ang = gt_angles[i]
        gt_composite_hmap = torch.sum(gt_hmap, dim=0).cpu().numpy()
        gt_heatmap_resized = cv2.resize(gt_composite_hmap, (img_w, img_h))
        gt_vis = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
        gt_vis = cv2.cvtColor(gt_vis, cv2.COLOR_GRAY2BGR)
        gt_vis = cv2.addWeighted(gt_vis, 0.3, cv2.applyColorMap((gt_heatmap_resized * 255).astype(np.uint8), cv2.COLORMAP_JET), 0.7, 0)

        # --- ì˜ˆì¸¡ ì‹œê°í™” ---
        pred_hmap = pred_heatmaps_batch[i].cpu()
        pred_ang = pred_angles_batch[i].cpu()
        pred_composite_hmap = torch.sum(pred_hmap, dim=0).numpy()
        pred_heatmap_resized = cv2.resize(pred_composite_hmap, (img_w, img_h))
        pred_vis = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
        pred_vis = cv2.cvtColor(pred_vis, cv2.COLOR_GRAY2BGR)
        pred_vis = cv2.addWeighted(pred_vis, 0.3, cv2.applyColorMap((pred_heatmap_resized * 255).astype(np.uint8), cv2.COLORMAP_JET), 0.7, 0)

        # --- í…ìŠ¤íŠ¸ ì¶”ê°€ ---
        gt_text = "GT Angles: " + ", ".join([f"{a:.2f}" for a in gt_ang.numpy()])
        pred_text = "Pred Angles: " + ", ".join([f"{a:.2f}" for a in pred_ang.numpy()])
        cv2.putText(gt_vis, "Ground Truth", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        cv2.putText(pred_vis, "Prediction", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        # --- ì´ë¯¸ì§€ ë³‘í•© ---
        comparison_image = cv2.hconcat([img_bgr, gt_vis, pred_vis])
        
        # wandb ë¡œê¹…ì„ ìœ„í•´ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
        final_image = Image.fromarray(cv2.cvtColor(comparison_image, cv2.COLOR_BGR2RGB))
        log_images.append(wandb.Image(final_image, caption=f"{gt_text}\n{pred_text}"))
        
    # wandbì— ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ë¡œê¹…
    wandb.log({title: log_images})
    model.train() # ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜
    
def train_one_epoch(model, loader, optimizer_kpt, optimizer_ang, crit_kpt, crit_ang, device, loss_weight_kpt=1.0, epoch_num=0):
    model.train()
    total_loss_kpt = 0
    total_loss_ang = 0
    first_batch = None # ì²« ë°°ì¹˜ë¥¼ ì €ì¥í•  ë³€ìˆ˜
    
    loop = tqdm(loader, desc=f"Epoch {epoch_num} [Train]")
    
    for i, (images, heatmaps, angles) in enumerate(loop):
        if i == 0: # âœ… ì²« ë²ˆì§¸ ë°°ì¹˜ ì €ì¥
            first_batch = (images.cpu(), heatmaps.cpu(), angles.cpu())

        images, heatmaps, angles = images.to(device), heatmaps.to(device), angles.to(device)
        
        pred_heatmaps, pred_angles = model(images)
        
        # --- Keypoint Head ì—…ë°ì´íŠ¸ ---
        optimizer_kpt.zero_grad()
        loss_kpt = crit_kpt(pred_heatmaps, heatmaps) * loss_weight_kpt
        loss_kpt.backward()
        optimizer_kpt.step()
        
        # --- Angle Head ì—…ë°ì´íŠ¸ ---
        optimizer_ang.zero_grad()
        loss_ang = crit_ang(pred_angles, angles)
        loss_ang.backward()
        optimizer_ang.step()
        
        # ì†ì‹¤ ê¸°ë¡
        total_loss_kpt += loss_kpt.item()
        total_loss_ang += loss_ang.item()
        
        # ì§„í–‰ë¥  í‘œì‹œì¤„ì— ê° ì†ì‹¤ ê°’ì„ ì—…ë°ì´íŠ¸
        loop.set_postfix(loss_ang=loss_ang.item(), loss_kpt=loss_kpt.item())
        
    # í‰ê·  ì†ì‹¤ ë°˜í™˜
    avg_loss_kpt = total_loss_kpt / len(loader)
    avg_loss_ang = total_loss_ang / len(loader)
    return avg_loss_kpt, avg_loss_ang, first_batch 

def validate(model, loader, crit_kpt, crit_ang, device, loss_weight_kpt=1.0, epoch_num=0):
    model.eval()
    total_loss = 0
    first_batch = None # ì²« ë°°ì¹˜ë¥¼ ì €ì¥í•  ë³€ìˆ˜
    
    loop = tqdm(loader, desc=f"Epoch {epoch_num} [Validate]", leave=False)
    
    with torch.no_grad():
        for i, (images, heatmaps, angles) in enumerate(loop):
            if i == 0: # âœ… ì²« ë²ˆì§¸ ë°°ì¹˜ ì €ì¥
                first_batch = (images.cpu(), heatmaps.cpu(), angles.cpu())
                
            images, heatmaps, angles = images.to(device), heatmaps.to(device), angles.to(device)
            
            pred_heatmaps, pred_angles = model(images)
            
            loss_kpt = crit_kpt(pred_heatmaps, heatmaps) * loss_weight_kpt
            loss_ang = crit_ang(pred_angles, angles)
            loss = loss_kpt + loss_ang
            
            total_loss += loss.item()
            loop.set_postfix(loss=loss.item())
            
    return total_loss / len(loader), first_batch

class RandomMasking(object):
    """
    PIL ì´ë¯¸ì§€ì— ë¬´ì‘ìœ„ ì‚¬ê°í˜• ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ëŠ” transform.
    """
    def __init__(self, num_masks=1, mask_size_ratio=(0.1, 0.3), mask_color='random'):
        assert isinstance(num_masks, int) and num_masks > 0
        assert isinstance(mask_size_ratio, tuple) and len(mask_size_ratio) == 2
        self.num_masks = num_masks
        self.mask_size_ratio = mask_size_ratio
        self.mask_color = mask_color

    def __call__(self, img):
        """
        Args:
            img (PIL Image): ì…ë ¥ ì´ë¯¸ì§€.
        Returns:
            PIL Image: ë§ˆìŠ¤í¬ê°€ ì ìš©ëœ ì´ë¯¸ì§€.
        """
        # PIL ì´ë¯¸ì§€ë¥¼ OpenCVê°€ ë‹¤ë£° ìˆ˜ ìˆëŠ” Numpy ë°°ì—´ë¡œ ë³€í™˜ (RGB ìˆœì„œ ìœ ì§€)
        img_np = np.array(img)
        h, w, _ = img_np.shape

        for _ in range(self.num_masks):
            # ë§ˆìŠ¤í¬ í¬ê¸° ê²°ì •
            mask_w = int(w * random.uniform(self.mask_size_ratio[0], self.mask_size_ratio[1]))
            mask_h = int(h * random.uniform(self.mask_size_ratio[0], self.mask_size_ratio[1]))
            
            # ë§ˆìŠ¤í¬ ìœ„ì¹˜ ê²°ì •
            x_start = random.randint(0, w - mask_w)
            y_start = random.randint(0, h - mask_h)
            
            # ë§ˆìŠ¤í¬ ìƒ‰ìƒ ê²°ì •
            if self.mask_color == 'black':
                color = (0, 0, 0)
            elif self.mask_color == 'white':
                color = (255, 255, 255)
            else: # 'random'
                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))
            
            # ì´ë¯¸ì§€ì— ë§ˆìŠ¤í¬ ì ìš©
            img_np[y_start:y_start+mask_h, x_start:x_start+mask_w] = color
        
        # ë‹¤ì‹œ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        return Image.fromarray(img_np)
# ==============================================================================
# 3. DDP ë° ì»´í¬ë„ŒíŠ¸ ì„¤ì • í•¨ìˆ˜
# ==============================================================================

def setup_ddp():
    """DDP í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ì´ˆê¸°í™”í•˜ê³  ë¡œì»¬ ë­í¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    dist.init_process_group(backend="nccl")
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    return local_rank

def cleanup_ddp():
    """DDP í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    dist.destroy_process_group()

def setup_components(hyperparameters, dataset_pairs, rank, world_size):
    """í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. (DDP ë²„ì „)"""
    model_name = hyperparameters['model_name']
    batch_size = hyperparameters['batch_size']
    val_split = hyperparameters['val_split']
    
    device = torch.device(f'cuda:{rank}')
    
    config = timm.create_model(model_name, pretrained=True,).default_cfg
    
    train_transform = transforms.Compose([
        transforms.Resize(config['input_size'][-2:]),
        transforms.ColorJitter(brightness=0.2, contrast=0.15, saturation=0.15, hue=0.05),
        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),
        transforms.RandomGrayscale(p=0.1),
        transforms.ToTensor(),
        transforms.RandomErasing(p=0.2, scale=(0.1, 0.2), ratio=(0.3, 2.0)),
        transforms.Normalize(mean=config['mean'], std=config['std'])
    ])

    # âœ… ê²€ì¦ ë° ì‹œê°í™”ìš© Transform (ì¦ê°• ì—†ìŒ)
    val_transform = transforms.Compose([
        transforms.Resize(config['input_size'][-2:]),
        transforms.ToTensor(),
        transforms.Normalize(mean=config['mean'], std=config['std'])
    ])
    
    indices = list(range(len(dataset_pairs)))
    train_size = int(len(indices) * (1 - val_split))
    
    # ëª¨ë“  GPUê°€ ë™ì¼í•œ ë¶„í• ì„ ì‚¬ìš©í•˜ë„ë¡ ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.
    np.random.seed(42)
    np.random.shuffle(indices)
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]

    train_dataset = RobotPoseDataset(pairs=[dataset_pairs[i] for i in train_indices], transform=train_transform)
    val_dataset = RobotPoseDataset(pairs=[dataset_pairs[i] for i in val_indices], transform=val_transform)
    
    # --- DDPìš© Sampler ì„¤ì • ---
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)
    
    def collate_fn(batch):
        batch = list(filter(lambda x: x[0] is not None, batch))
        return torch.utils.data.dataloader.default_collate(batch) if batch else (None, None, None)

    # shuffle=False (Samplerê°€ ì…”í”Œì„ ë‹´ë‹¹)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8, pin_memory=True, sampler=train_sampler, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=8, pin_memory=True, sampler=val_sampler, collate_fn=collate_fn)
    
    model = DINOv2PoseEstimator(model_name).to(device)
    
    for param in model.backbone.parameters():
        param.requires_grad = False
        
    return model, train_loader, val_loader, config, val_transform, train_sampler


# ==============================================================================
# 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================
def main():
    """ë©”ì¸ í•™ìŠµ ë¡œì§"""
    local_rank = setup_ddp()
    world_size = dist.get_world_size()
    
    # --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ---
    hyperparameters = {
        'model_name': 'vit_base_patch14_dinov2.lvd142m',
        'batch_size': 240, # GPUë‹¹ ë°°ì¹˜ì‚¬ì´ì¦ˆ (ì „ì²´ 540 / GPU 3ê°œ)
        'num_epochs': 100,
        'val_split': 0.1,
        'loss_weight_kpt': 1000.0,
        'lr_kpt': 0.0001,
        'lr_ang': 0.0001,
    }

    # --- ë°ì´í„° ë¡œë“œ (ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰, ë¡œê·¸ëŠ” rank 0ì—ì„œë§Œ) ---
    if local_rank == 0:
        print("--- Loading and preparing dataset pairs ---")
    
    CSV_PATHS = [
        '../dataset/DREAM_real/panda-3cam_azure/panda-3cam_azure_matched_data.csv',
        '../dataset/DREAM_real/panda-3cam_kinect360/panda-3cam_kinect360_matched_data.csv',
        '../dataset/DREAM_real/panda-3cam_realsense/panda-3cam_realsense_matched_data.csv',
        '../dataset/DREAM_real/panda-orb/panda-orb_matched_data.csv',
    ]
    all_dfs = [pd.read_csv(path) for path in CSV_PATHS if os.path.exists(path)]
    if not all_dfs:
        if local_rank == 0: print("âŒ ERROR: No CSV files were loaded.")
        return
    total_csv = pd.concat(all_dfs, ignore_index=True)
    dataset_pairs = [{'image_path': row['image_path'], 'joint_angles': [row[f'joint_{j}'] for j in range(1, NUM_ANGLES + 1)], 'keypoints_2d': {name: [row[f'kpt_{name}_proj_x'], row[f'kpt_{name}_proj_y']] for name in REQUIRED_KEYPOINTS}} for _, row in total_csv.iterrows()]
    
    if local_rank == 0:
        print(f"âœ… All CSV files merged. Total pairs: {len(dataset_pairs)}")

    # --- í•™ìŠµ ì»´í¬ë„ŒíŠ¸ ì„¤ì • ---
    model, train_loader, val_loader, config, val_transform, train_sampler = setup_components(
        hyperparameters, dataset_pairs, local_rank, world_size
    )
    
    model = DDP(model, device_ids=[local_rank])
    
    crit_kpt = nn.MSELoss()
    crit_ang = nn.SmoothL1Loss(beta=1.0)
    optimizer_kpt = torch.optim.AdamW(model.module.keypoint_head.parameters(), lr=hyperparameters['lr_kpt'])
    optimizer_ang = torch.optim.AdamW(model.module.angle_head.parameters(), lr=hyperparameters['lr_ang'])
    scheduler_kpt = CosineAnnealingLR(optimizer_kpt, T_max=hyperparameters['num_epochs'], eta_min=1e-6)
    scheduler_ang = CosineAnnealingLR(optimizer_ang, T_max=hyperparameters['num_epochs'], eta_min=1e-6)

    # --- WandB ë° í•™ìŠµ ì‹œì‘ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ) ---
    if local_rank == 0:
        run = wandb.init(project="robot-pose-estimation", config=hyperparameters, name=f"DREAM_DDP_run_{time.strftime('%Y%m%d_%H%M%S')} BAEK")
        wandb.watch(model, log="all", log_freq=100)
        print("\n--- Starting Training ---")
        # visualize_dataset_sample(train_loader.dataset, config, num_samples=3)

    best_val_loss = float('inf')
    for epoch in range(hyperparameters['num_epochs']):
        train_sampler.set_epoch(epoch)
        
        # âœ… ë°˜í™˜ê°’ì— first_train_batch ì¶”ê°€
        train_loss_kpt, train_loss_ang, first_train_batch = train_one_epoch(
            model, train_loader, optimizer_kpt, optimizer_ang, crit_kpt, crit_ang, 
            torch.device(f'cuda:{local_rank}'), hyperparameters['loss_weight_kpt'], epoch+1
        )
        # âœ… ë°˜í™˜ê°’ì— first_val_batch ì¶”ê°€
        val_loss, first_val_batch = validate(
            model, val_loader, crit_kpt, crit_ang, 
            torch.device(f'cuda:{local_rank}'), hyperparameters['loss_weight_kpt'], epoch+1
        )
        
        scheduler_kpt.step()
        scheduler_ang.step()

        if local_rank == 0:
            current_lr_kpt = optimizer_kpt.param_groups[0]['lr']
            current_lr_ang = optimizer_ang.param_groups[0]['lr']
            print(f"Epoch {epoch+1}/{hyperparameters['num_epochs']} -> Train Losses [Kpt: {train_loss_kpt:.6f}, Ang: {train_loss_ang:.6f}], Val Loss: {val_loss:.6f}, LR [Kpt: {current_lr_kpt:.6f}, Ang: {current_lr_ang:.6f}]")
            
            # âœ… WandBì— ìˆ«ì ë©”íŠ¸ë¦­ ë¡œê¹…
            wandb.log({
                "epoch": epoch + 1, 
                "train_loss_kpt": train_loss_kpt, 
                "train_loss_ang": train_loss_ang, 
                "avg_val_loss": val_loss, 
                "lr_kpt": current_lr_kpt, 
                "lr_ang": current_lr_ang
            })

            # âœ… WandBì— ì´ë¯¸ì§€ ë¡œê¹…
            # ê²€ì¦ ìƒ˜í”Œì€ ë§¤ ì—í¬í¬ë§ˆë‹¤ ë¡œê¹…
            if first_val_batch[0] is not None:
                log_predictions_to_wandb(model.module, first_val_batch[0], first_val_batch[1], first_val_batch[2], 
                                         torch.device(f'cuda:{local_rank}'), config, "Validation Predictions")
            
            # í•™ìŠµ ìƒ˜í”Œì€ 10 ì—í¬í¬ë§ˆë‹¤ ë¡œê¹… (ë„ˆë¬´ ìì£¼ ë¡œê¹…í•˜ëŠ” ê²ƒì„ ë°©ì§€)
            if (epoch + 1) % 10 == 0 and first_train_batch[0] is not None:
                log_predictions_to_wandb(model.module, first_train_batch[0], first_train_batch[1], first_train_batch[2], 
                                         torch.device(f'cuda:{local_rank}'), config, "Train Predictions")

            # (ê¸°ì¡´ ëª¨ë¸ ì €ì¥ ë¡œì§ì€ ë™ì¼)
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                print(f"  -> ğŸ‰ New best model saved with validation loss: {best_val_loss:.6f}")
                state_to_save = model.module.state_dict()
                save_thread = threading.Thread(target=torch.save, args=(state_to_save, 'best_pose_estimator_model.pth'))
                save_thread.start()

    if local_rank == 0:
        if 'save_thread' in locals() and save_thread.is_alive():
            save_thread.join()
        run.finish()
        
    cleanup_ddp()

if __name__ == '__main__':
    main()