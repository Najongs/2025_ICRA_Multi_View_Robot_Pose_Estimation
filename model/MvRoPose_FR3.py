import os
import glob
import json
import numpy as np
import random
import wandb
import threading

import cv2
import math
from scipy.spatial.transform import Rotation as R
from tqdm import tqdm 

import timm
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from torch.optim.lr_scheduler import CosineAnnealingLR # ìƒë‹¨ì— ì¶”ê°€
import kornia.augmentation as K


from transformers import AutoModel
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
import pandas as pd

NUM_ANGLES = 7
NUM_JOINTS = 8
FEATURE_DIM = 768
HEATMAP_SIZE = (128, 128)

MODEL_NAME = 'facebook/dinov3-vitb16-pretrain-lvd1689m'
MAX_VIEWS_PER_GROUP = 8

# â–¼â–¼â–¼ [í•µì‹¬ ìˆ˜ì •] ê·¸ë£¹í•‘ ë¡œì§ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬ â–¼â–¼â–¼
def perform_grouping(df, tolerance, max_views):
    """ì£¼ì–´ì§„ tolerance ê°’ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ì„ ê·¸ë£¹í•‘í•©ë‹ˆë‹¤."""
    groups = []
    if not df.empty:
        current_views = []
        for _, row in df.iterrows():
            if not current_views:
                current_views.append(row)
                continue
            start_time = current_views[0]['robot_timestamp']
            if (row['robot_timestamp'] - start_time > tolerance) or (len(current_views) >= max_views):
                joint_angles = [current_views[0][f'position_fr3_joint{j}'] for j in range(1, NUM_ANGLES + 1)]
                image_paths = [{'image_path': view['image_path']} for view in current_views]
                groups.append({'views': image_paths, 'joint_angles': joint_angles})
                current_views = [row]
            else:
                current_views.append(row)
        if current_views:
            joint_angles = [current_views[0][f'position_fr3_joint{j}'] for j in range(1, NUM_ANGLES + 1)]
            image_paths = [{'image_path': view['image_path']} for view in current_views]
            groups.append({'views': image_paths, 'joint_angles': joint_angles})
    return groups

# ==============================================================================
# í—¬í¼ í•¨ìˆ˜ (Ground Truth ìƒì„±ìš©)
# ==============================================================================

def create_gt_heatmap(keypoint_2d, HEATMAP_SIZE, sigma):
    """2D ì¢Œí‘œë¡œë¶€í„° ê°€ìš°ì‹œì•ˆ íˆíŠ¸ë§µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    H, W = HEATMAP_SIZE
    x, y = keypoint_2d
    xx, yy = np.meshgrid(np.arange(W), np.arange(H))
    dist_sq = (xx - x)**2 + (yy - y)**2
    heatmap = np.exp(-dist_sq / (2 * sigma**2))
    heatmap[heatmap < np.finfo(float).eps * heatmap.max()] = 0
    return heatmap

def get_modified_dh_matrix(a, d, alpha, theta):
    """Modified DH íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜ í–‰ë ¬ Të¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    alpha_rad, theta_rad = math.radians(alpha), math.radians(theta)
    cos_th, sin_th = np.cos(theta_rad), np.sin(theta_rad)
    cos_al, sin_al = np.cos(alpha_rad), np.sin(alpha_rad)
    
    # Craig's Modified DH Convention
    T = np.array([
        [cos_th, -sin_th, 0, a],
        [sin_th * cos_al, cos_th * cos_al, -sin_al, -d * sin_al],
        [sin_th * sin_al, cos_th * sin_al,  cos_al,  d * cos_al],
        [0, 0, 0, 1]
    ])
    return T

def angle_to_joint_coordinate(joint_angles, selected_view):
    """[Forward Kinematics] 7ê°œ ê´€ì ˆ ê°ë„ë¥¼ 8ê°œ(ë² ì´ìŠ¤ í¬í•¨)ì˜ 3D ê³µê°„ ì¢Œí‘œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # Franka Research 3 ë¡œë´‡ì˜ DH íŒŒë¼ë¯¸í„° (ë‹¨ìœ„: ë¯¸í„°, ë„)
    fr3_dh_parameters = [
        {'a': 0,       'd': 0.333, 'alpha': 0,   'theta_offset': 0}, # Joint 1
        {'a': 0,       'd': 0,     'alpha': -90, 'theta_offset': 0}, # Joint 2
        {'a': 0,       'd': 0.316, 'alpha': 90,  'theta_offset': 0}, # Joint 3
        {'a': 0.0825,  'd': 0,     'alpha': 90,  'theta_offset': 0}, # Joint 4
        {'a': -0.0825, 'd': 0.384, 'alpha': -90, 'theta_offset': 0}, # Joint 5
        {'a': 0,       'd': 0,     'alpha': 90,  'theta_offset': 0}, # Joint 6
        {'a': 0.088,   'd': 0,     'alpha': 90,  'theta_offset': 0}, # Joint 7
        {'a': 0,       'd': 0.107, 'alpha': 0,   'theta_offset': 0}  # Flange (End-effector base)
    ]
    
    # ì¹´ë©”ë¼ ë·°ì— ë”°ë¥¸ ë¡œë´‡ ë² ì´ìŠ¤ì˜ ì¢Œí‘œê³„ ë³´ì •
    view_rotations = {
        'view1': R.from_euler('zyx', [90, 180, 0], degrees=True),
        'view2': R.from_euler('zyx', [90, 180, 0], degrees=True),
        'view3': R.from_euler('zyx', [90, 180, 0], degrees=True),
        'view4': R.from_euler('zyx', [90, 180, 0], degrees=True)
    }
    
    T_cumulative = np.eye(4)
    if selected_view in view_rotations:
        T_cumulative[:3, :3] = view_rotations[selected_view].as_matrix()

    # J0(ë² ì´ìŠ¤) ì¢Œí‘œëŠ” ì›ì 
    joint_coords_3d = [np.array([0, 0, 0])] 
    
    origin_point = np.array([0, 0, 0, 1])
    # ê° ê´€ì ˆ ê°ë„ë¥¼ ìˆœì„œëŒ€ë¡œ ì ìš©í•˜ì—¬ ë³€í™˜ í–‰ë ¬ì„ ëˆ„ì  ê³±ì…ˆ
    for i, angle_rad in enumerate(joint_angles):
        params = fr3_dh_parameters[i]
        theta_deg = math.degrees(angle_rad) + params['theta_offset']
        T_i = get_modified_dh_matrix(params['a'], params['d'], params['alpha'], theta_deg)
        T_cumulative = T_cumulative @ T_i
        
        # ëˆ„ì ëœ ë³€í™˜ í–‰ë ¬ì„ í†µí•´ í˜„ì¬ ê´€ì ˆì˜ 3D ì¢Œí‘œ ê³„ì‚°
        joint_pos_3d = (T_cumulative @ origin_point)[:3]
        joint_coords_3d.append(joint_pos_3d)
        
    return np.array(joint_coords_3d, dtype=np.float32)

def joint_coordinate_to_pixel_plane(coords_3d, aruco_result, camera_matrix, dist_coeffs):
    """[3D-2D íˆ¬ì˜] 3D ì¢Œí‘œë¥¼ ArUco ë§ˆì»¤ ê¸°ë°˜ ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°ë¡œ 2D í”½ì…€ í‰ë©´ì— íˆ¬ì˜í•©ë‹ˆë‹¤."""
    # ì¹´ë©”ë¼ ì™¸ë¶€ íŒŒë¼ë¯¸í„° (Extrinsics): ì›”ë“œ ì¢Œí‘œê³„ -> ì¹´ë©”ë¼ ì¢Œí‘œê³„ ë³€í™˜
    rvec = np.array([aruco_result['rvec_x'], aruco_result['rvec_y'], aruco_result['rvec_z']])
    tvec = np.array([aruco_result['tvec_x'], aruco_result['tvec_y'], aruco_result['tvec_z']])
    
    # OpenCVì˜ projectPoints í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 3D í¬ì¸íŠ¸ë¥¼ 2D ì´ë¯¸ì§€ í‰ë©´ìœ¼ë¡œ íˆ¬ì˜
    pixel_coords, _ = cv2.projectPoints(coords_3d, rvec, tvec, camera_matrix, dist_coeffs)
    return pixel_coords.reshape(-1, 2)

# ==============================================================================
# 2. ë©€í‹°ë·° ë°ì´í„°ì…‹ í´ë˜ìŠ¤
# ==============================================================================
class RobotPoseDataset(Dataset):
    def __init__(self, groups, transform=None, HEATMAP_SIZE=(128, 128), sigma=5.0):
        self.groups = groups
        self.transform = transform
        self.heatmap_size = HEATMAP_SIZE
        self.sigma = sigma

        print("Loading and preprocessing metadata...")
        # ArUcoì™€ Calibration ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì—¬ I/O ë³‘ëª©ì„ ì¤„ì…ë‹ˆë‹¤.
        self.aruco_lookup = {}
        for pose_name in ['pose1', 'pose2']:
            aruco_path = f'../dataset/franka_research3/{pose_name}_aruco_pose_summary.json'
            with open(aruco_path, 'r') as f:
                for item in json.load(f):
                    self.aruco_lookup[f"{pose_name}_{item['view']}_{item['cam']}"] = item
        
        self.calib_lookup = {}
        calib_dir = "../dataset/franka_research3/franka_research3_calib_cam_from_conf"
        for calib_path in glob.glob(os.path.join(calib_dir, "*.json")):
            filename = os.path.basename(calib_path).replace("_calib.json", "")
            with open(calib_path, 'r') as f:
                self.calib_lookup[filename] = json.load(f)
        
        self.serial_to_view = {
            '41182735': "view1", '49429257': "view2",
            '44377151': "view3", '49045152': "view4"
        }
        print("âœ… Metadata loaded.")

    def __len__(self):
        return len(self.groups)

    def __getitem__(self, idx):
        group = self.groups[idx]
        first_image_path = group.get('views', [{}])[0].get('image_path', f"group_{idx}")
        try:
            gt_angles = torch.tensor(group['joint_angles'], dtype=torch.float32)
            image_dict, gt_heatmaps_dict = {}, {}

            for view_data in group['views']:
                image_path = view_data['image_path']
                
                # --- ê²½ë¡œ ë¶„ì„ ---
                filename = os.path.basename(image_path)
                parts = filename.split('_')
                serial_str, cam_type_str = parts[1], parts[2]
                selected_cam = cam_type_str + "cam"
                selected_view = self.serial_to_view[serial_str]
                
                # â–¼â–¼â–¼ [í•µì‹¬ ë²„ê·¸ ìˆ˜ì •] view_keyë¥¼ ê³ ìœ í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. â–¼â–¼â–¼
                # (ê¸°ì¡´) view_key = serial_str 
                # (ë³€ê²½) view_key = f"{serial_str}_{cam_type_str}"  # ì˜ˆ: '41182735_left'
                view_key = f"{serial_str}_{cam_type_str}"

                # --- ì´í›„ ë¡œì§ì€ ë™ì¼ ---
                calib_key = f"{selected_view}_{serial_str}_{selected_cam}"
                calib = self.calib_lookup[calib_key]
                camera_matrix = np.array(calib["camera_matrix"], dtype=np.float32)
                dist_coeffs = np.array(calib["distortion_coeffs"], dtype=np.float32)
                pose_name = 'pose1' if 'pose1' in image_path else 'pose2'
                aruco_key = f"{pose_name}_{selected_view}_{selected_cam}"
                aruco_result = self.aruco_lookup[aruco_key]

                img_bgr = cv2.imread(image_path)
                if img_bgr is None: raise FileNotFoundError(f"Failed to read image")
                img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
                undistorted_np = cv2.undistort(img_rgb, camera_matrix, dist_coeffs)
                
                joint_coords_3d = angle_to_joint_coordinate(group['joint_angles'], selected_view)
                pixel_coords = joint_coordinate_to_pixel_plane(joint_coords_3d, aruco_result, camera_matrix, np.zeros_like(dist_coeffs))
                
                h, w, _ = undistorted_np.shape
                scaled_kpts = pixel_coords * [self.heatmap_size[1]/w, self.heatmap_size[0]/h]
                
                heatmaps_np = np.zeros((NUM_JOINTS, *self.heatmap_size), dtype=np.float32)
                for i in range(NUM_JOINTS):
                    heatmaps_np[i] = create_gt_heatmap(scaled_kpts[i], self.heatmap_size, self.sigma)
                
                image_dict[view_key] = self.transform(Image.fromarray(undistorted_np))
                gt_heatmaps_dict[view_key] = torch.from_numpy(heatmaps_np)

            return image_dict, gt_heatmaps_dict, gt_angles

        except Exception as e:
            # print(f"âš ï¸ Warning: Skipping group {idx} (e.g., {os.path.basename(first_image_path)}) due to error: {e}")
            return None, None, None

import torch
import torchvision.transforms as transforms
from transformers import AutoImageProcessor
import matplotlib.pyplot as plt
import pandas as pd

# ==============================================================================
# 1. ì‹œê°í™”ë¥¼ ìœ„í•œ Transform ë° ë°ì´í„°ì…‹ ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼)
# ==============================================================================



# ==============================================================================
# 2. ì‹œê°í™” í•¨ìˆ˜ ì •ì˜ (ìˆ˜ì •)
# ==============================================================================

def visualize_samples_by_group_size(groups, transform, mean, std):
    """
    ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ê·¸ë£¹ í¬ê¸°(8, 7, 6...)ì— ëŒ€í•´
    ê°ê° í•˜ë‚˜ì˜ ìƒ˜í”Œì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    print("\n--- Visualizing One Sample For Each Group Size ---")
    
    # ê·¸ë£¹ í¬ê¸°ë³„ë¡œ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
    groups_by_size = {}
    for group in groups:
        size = len(group['views'])
        if size not in groups_by_size:
            groups_by_size[size] = []
        groups_by_size[size].append(group)

    # ê·¸ë£¹ í¬ê¸°ê°€ í° ìˆœì„œëŒ€ë¡œ (8, 7, 6...) ì •ë ¬
    sorted_sizes = sorted(groups_by_size.keys(), reverse=True)

    # ê° ê·¸ë£¹ í¬ê¸°ì— ëŒ€í•´ ë°˜ë³µ
    for size in sorted_sizes:
        # í•´ë‹¹ í¬ê¸°ì˜ ê·¸ë£¹ ì¤‘ í•˜ë‚˜ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ
        sample_group = random.choice(groups_by_size[size])
        
        # ì„ì‹œ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ __getitem__ ë¡œì§ í™œìš©
        temp_dataset = RobotPoseDataset(groups=[sample_group], transform=transform)
        image_dict, gt_heatmaps_dict, gt_angles = temp_dataset[0]

        if image_dict is None:
            print(f"Could not process sample for group size {size}. Skipping.")
            continue
            
        # --- ì‹œê°í™” ë¡œì§ (ê¸°ì¡´ê³¼ ê±°ì˜ ë™ì¼) ---
        num_views = len(image_dict)
        fig, axes = plt.subplots(2, num_views, figsize=(6 * num_views, 10))
        if num_views == 1: axes = np.expand_dims(axes, axis=1)

        angle_str = ", ".join([f"{a:.2f}" for a in gt_angles.numpy()])
        fig.suptitle(f"Sample for Group Size: {num_views} | GT Angles: [{angle_str}]", fontsize=16)

        for j, view_key in enumerate(image_dict.keys()):
            img_tensor = image_dict[view_key]
            img_np = img_tensor.numpy().transpose(1, 2, 0)
            img_np = np.array(std) * img_np + np.array(mean)
            img_np = np.clip(img_np, 0, 1)
            H, W, _ = img_np.shape

            gt_heatmaps = gt_heatmaps_dict[view_key]
            composite_heatmap = torch.sum(gt_heatmaps, dim=0).numpy()
            heatmap_resized = cv2.resize(composite_heatmap, (W, H))

            keypoints = []
            h_map, w_map = gt_heatmaps.shape[1:]
            for k in range(gt_heatmaps.shape[0]):
                y, x = np.unravel_index(torch.argmax(gt_heatmaps[k]).numpy(), (h_map, w_map))
                keypoints.append([x * (W / w_map), y * (H / h_map)])
            keypoints = np.array(keypoints)

            ax = axes[0, j]
            ax.imshow(img_np, alpha=0.7)
            ax.imshow(heatmap_resized, cmap='jet', alpha=0.3)
            ax.set_title(f"View: {view_key} (Heatmap)")
            ax.axis('off')

            ax = axes[1, j]
            ax.imshow(img_np)
            ax.scatter(keypoints[:, 0], keypoints[:, 1], c='lime', s=40, edgecolors='black', linewidth=1)
            ax.set_title(f"View: {view_key} (Keypoints)")
            ax.axis('off')
            
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.show()

# ==============================================================================
# 3. ì‹œê°í™” ì‹¤í–‰
# ==============================================================================
# ìµœì¢…ì ìœ¼ë¡œ í•„í„°ë§ëœ dataset_groupsë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ

class DINOv3Backbone(nn.Module):
    """
    Hugging Face transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ DINOv3 ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    ì‚¬ì „ì— ì •ê·œí™”ëœ ì´ë¯¸ì§€ í…ì„œ ë°°ì¹˜ë¥¼ ì…ë ¥ë°›ì•„ íŒ¨ì¹˜ í† í°ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    def __init__(self, model_name=MODEL_NAME): # ViT-Base ëª¨ë¸ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        super().__init__()
        # ì‚¬ì „ í•™ìŠµëœ DINOv3 ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        self.model = AutoModel.from_pretrained(model_name)
        # âš ï¸ ì°¸ê³ : ëª¨ë¸ì„ íŠ¹ì • ì¥ì¹˜(.to('cuda'))ë¡œ ë³´ë‚´ëŠ” ì½”ë“œëŠ”
        # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

    def forward(self, image_tensor_batch):
        """
        Args:
            image_tensor_batch (torch.Tensor): (B, C, H, W) í˜•íƒœì˜ ì •ê·œí™”ëœ ì´ë¯¸ì§€ í…ì„œ
        """
        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.
        with torch.no_grad():
            # Hugging Face ëª¨ë¸ì€ 'pixel_values'ë¼ëŠ” í‚¤ì›Œë“œ ì¸ìë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.
            outputs = self.model(pixel_values=image_tensor_batch)

        last_hidden_state = outputs.last_hidden_state
        
        # í´ë˜ìŠ¤ í† í°(CLS)ì„ ì œì™¸í•œ íŒ¨ì¹˜ í† í°ë“¤ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        patch_tokens = last_hidden_state[:, 1:, :]
        
        return patch_tokens

class JointAngleHead(nn.Module):
    def __init__(self, input_dim=FEATURE_DIM, num_angles=NUM_ANGLES, num_queries=4, nhead=8, num_decoder_layers=2):
        super().__init__()
        
        # 1. "ë¡œë´‡ í¬ì¦ˆì— ëŒ€í•´ ì§ˆë¬¸í•˜ëŠ”" í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ í† í° ìƒì„±
        self.pose_queries = nn.Parameter(torch.randn(1, num_queries, input_dim))
        
        # 2. PyTorchì˜ í‘œì¤€ Transformer Decoder ë ˆì´ì–´ ì‚¬ìš©
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=input_dim, 
            nhead=nhead, 
            dim_feedforward=input_dim * 4, # ì¼ë°˜ì ì¸ ì„¤ì •
            dropout=0.1, 
            activation='gelu',
            batch_first=True  # (batch, seq, feature) ì…ë ¥ì„ ìœ„í•¨
        )
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)
        
        # 3. ìµœì¢… ê°ë„ ì˜ˆì¸¡ì„ ìœ„í•œ MLP
        # ë””ì½”ë”ë¥¼ ê±°ì¹œ ëª¨ë“  ì¿¼ë¦¬ í† í°ì˜ ì •ë³´ë¥¼ ì‚¬ìš©
        self.angle_predictor = nn.Sequential(
            nn.LayerNorm(input_dim * num_queries),
            nn.Linear(input_dim * num_queries, 512),
            nn.GELU(),
            nn.LayerNorm(512),
            nn.Linear(512, 256),
            nn.GELU(),
            nn.LayerNorm(256),
            nn.Linear(256, num_angles)
        )

    def forward(self, fused_features):
        # fused_features: DINOv2ì˜ íŒ¨ì¹˜ í† í°ë“¤ (B, Num_Patches, Dim)
        # self.pose_queries: í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ (1, Num_Queries, Dim)
        
        # ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ ì¿¼ë¦¬ë¥¼ ë³µì œ
        b = fused_features.size(0)
        queries = self.pose_queries.repeat(b, 1, 1)
        
        # Transformer Decoder ì—°ì‚°
        # ì¿¼ë¦¬(queries)ê°€ ì´ë¯¸ì§€ íŠ¹ì§•(fused_features)ì— ì–´í…ì…˜ì„ ìˆ˜í–‰í•˜ì—¬
        # í¬ì¦ˆì™€ ê´€ë ¨ëœ ì •ë³´ë¡œ ìì‹ ì˜ ê°’ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        attn_output = self.transformer_decoder(tgt=queries, memory=fused_features)
        
        # ì—…ë°ì´íŠ¸ëœ ì¿¼ë¦¬ í† í°ë“¤ì„ í•˜ë‚˜ë¡œ í¼ì³ì„œ MLPì— ì „ë‹¬
        output_flat = attn_output.flatten(start_dim=1)
        
        return self.angle_predictor(output_flat)

class MultiViewFusion(nn.Module):
    """
    Latent Query ê¸°ë°˜ì˜ Multi-view Fusion ëª¨ë“ˆ.
    """
    def __init__(self, feature_dim=FEATURE_DIM, num_heads=8, dropout=0.1, num_queries=16, num_layers=2):
        super().__init__()
        # ì”¬ ì „ì²´ì˜ ì •ë³´ë¥¼ ìš”ì•½í•  í•™ìŠµ ê°€ëŠ¥í•œ ê¸€ë¡œë²Œ ì¿¼ë¦¬
        self.global_queries = nn.Parameter(torch.randn(1, num_queries, feature_dim))
        
        # Cross-Attention + Self-Attentionìœ¼ë¡œ êµ¬ì„±ëœ Transformer Decoder ë ˆì´ì–´
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=feature_dim, nhead=num_heads, dim_feedforward=feature_dim * 4,
            dropout=dropout, activation='gelu', batch_first=True
        )
        self.fusion_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)

    def forward(self, view_features: list):
        # 1. ëª¨ë“  ë·°ì˜ í† í°ë“¤ì„ ì‹œí€€ìŠ¤ ì°¨ì›ì—ì„œ í•˜ë‚˜ë¡œ í•©ì¹¨
        all_view_tokens = torch.cat(view_features, dim=1)
        b = all_view_tokens.size(0)
        
        # 2. ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ ê¸€ë¡œë²Œ ì¿¼ë¦¬ ë³µì œ
        queries = self.global_queries.repeat(b, 1, 1)
        
        # 3. Decoderë¥¼ í†µí•´ ì¿¼ë¦¬ê°€ ëª¨ë“  ë·°ì˜ ì •ë³´ë¥¼ ìš”ì•½í•˜ë„ë¡ í•¨
        # ì¿¼ë¦¬ê°€ Key/Valueì¸ all_view_tokensì— Cross-Attentionì„ ìˆ˜í–‰í•˜ê³ ,
        # ì´í›„ ì¿¼ë¦¬ë“¤ë¼ë¦¬ Self-Attentionì„ ìˆ˜í–‰í•˜ë©° ì •ë³´ë¥¼ ì •ì œí•¨
        fused_queries = self.fusion_decoder(tgt=queries, memory=all_view_tokens)
        
        return fused_queries

class TokenFuser(nn.Module):
    """
    ViTì˜ íŒ¨ì¹˜ í† í°(1D ì‹œí€€ìŠ¤)ì„ CNNì´ ì‚¬ìš©í•˜ê¸° ì¢‹ì€ 2D íŠ¹ì§• ë§µìœ¼ë¡œ ë³€í™˜í•˜ê³  ì •ì œí•©ë‹ˆë‹¤.
    """
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.projection = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.refine_blocks = nn.Sequential(
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.GELU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
        self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)
    def forward(self, x):
        # x: (B, D, H, W) í˜•íƒœë¡œ reshapeëœ í† í° ë§µ
        projected = self.projection(x)
        refined = self.refine_blocks(projected)
        residual = self.residual_conv(x)
        return torch.nn.functional.gelu(refined + residual)

class LightCNNStem(nn.Module):
    def __init__(self):
        super().__init__()
        # ê°„ë‹¨í•œ CNN ë¸”ë¡ êµ¬ì„±
        self.conv_block1 = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1, bias=False), # í•´ìƒë„ 1/2
            nn.BatchNorm2d(16),
            nn.GELU(),
            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=False), # í•´ìƒë„ 1/4
            nn.BatchNorm2d(32),
            nn.GELU()
        )
        self.conv_block2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False), # í•´ìƒë„ 1/8
            nn.BatchNorm2d(64),
            nn.GELU()
        )
        
    def forward(self, x):
        # x: ì›ë³¸ ì´ë¯¸ì§€ í…ì„œ ë°°ì¹˜ (B, 3, H, W)
        feat_4 = self.conv_block1(x)  # 1/4 ìŠ¤ì¼€ì¼ íŠ¹ì§•
        feat_8 = self.conv_block2(feat_4) # 1/8 ìŠ¤ì¼€ì¼ íŠ¹ì§•
        return feat_4, feat_8 # ë‹¤ë¥¸ í•´ìƒë„ì˜ íŠ¹ì§•ë“¤ì„ ë°˜í™˜

class FusedUpsampleBlock(nn.Module):
    """
    ì—…ìƒ˜í”Œë§ëœ íŠ¹ì§•ê³¼ CNN ìŠ¤í…œì˜ ê³ í•´ìƒë„ íŠ¹ì§•(ìŠ¤í‚µ ì—°ê²°)ì„ ìœµí•©í•˜ëŠ” ë¸”ë¡.
    """
    def __init__(self, in_channels, skip_channels, out_channels):
        super().__init__()
        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.refine_conv = nn.Sequential(
            nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.GELU(),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.GELU()
        )

    def forward(self, x, skip_feature):
        x = self.upsample(x)
        
        # âœ… í•´ê²°ì±…: skip_featureë¥¼ xì˜ í¬ê¸°ì— ê°•ì œë¡œ ë§ì¶¥ë‹ˆë‹¤.
        # ----------------------------------------------------------------------
        # ë‘ í…ì„œì˜ ë†’ì´ì™€ ë„ˆë¹„ê°€ ë‹¤ë¥¼ ê²½ìš°, skip_featureë¥¼ xì˜ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•©ë‹ˆë‹¤.
        if x.shape[-2:] != skip_feature.shape[-2:]:
            skip_feature = F.interpolate(
                skip_feature, 
                size=x.shape[-2:], # target H, W
                mode='bilinear', 
                align_corners=False
            )
        # ----------------------------------------------------------------------
        
        # ì´ì œ ë‘ í…ì„œì˜ í¬ê¸°ê°€ ê°™ì•„ì¡Œìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ í•©ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        fused = torch.cat([x, skip_feature], dim=1)
        return self.refine_conv(fused)
    
class UNetViTKeypointHead(nn.Module):
    def __init__(self, input_dim=768, num_joints=7, heatmap_size=(128, 128)):
        super().__init__()
        self.heatmap_size = heatmap_size
        self.token_fuser = TokenFuser(input_dim, 256)
        self.decoder_block1 = FusedUpsampleBlock(in_channels=256, skip_channels=64, out_channels=128)
        self.decoder_block2 = FusedUpsampleBlock(in_channels=128, skip_channels=32, out_channels=64)
        self.final_upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        self.heatmap_predictor = nn.Conv2d(64, num_joints, kernel_size=3, padding=1)

    def forward(self, dino_features, cnn_features):
        cnn_feat_4, cnn_feat_8 = cnn_features

        # 1. DINOv3 í† í°ì„ í‘œì¤€ ViT íŒ¨ì¹˜ ê°œìˆ˜ì¸ 196ê°œë¡œ ì˜ë¼ë‚´ê³  2D ë§µìœ¼ë¡œ ë³€í™˜
        num_patches_to_keep = 196
        dino_features_sliced = dino_features[:, :num_patches_to_keep, :]
        
        b, n, d = dino_features_sliced.shape
        h = w = int(n**0.5)
        x = dino_features_sliced.permute(0, 2, 1).reshape(b, d, h, w)

        x = self.token_fuser(x)

        # 2. ë””ì½”ë” ì—…ìƒ˜í”Œë§ & ìœµí•©
        x = self.decoder_block1(x, cnn_feat_8)
        x = self.decoder_block2(x, cnn_feat_4)
        
        # 3. ìµœì¢… í•´ìƒë„ë¡œ ì—…ìƒ˜í”Œë§ ë° ì˜ˆì¸¡
        x = self.final_upsample(x)
        heatmaps = self.heatmap_predictor(x)
        
        return F.interpolate(heatmaps, size=self.heatmap_size, mode='bilinear', align_corners=False)
    
class DINOv3PoseEstimator(nn.Module):
    """
    [ì „ì²´ ì•„í‚¤í…ì²˜ (Overall Architecture)]
    Multi-view ì´ë¯¸ì§€ë“¤ì„ ì…ë ¥ë°›ì•„, ê° ë·°ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ìœµí•©í•˜ì—¬
    í•˜ë‚˜ì˜ í†µí•©ëœ ê´€ì ˆ ê°ë„(global pose)ì™€ ê° ë·°ì— ëŒ€í•œ í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ(local keypoints)ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """
    def __init__(self, model_name=MODEL_NAME, num_joints=NUM_JOINTS, num_angles=NUM_ANGLES, max_views=10):
        super().__init__()
        
        # 1. ë°±ë³¸: ê³ ì°¨ì› ì˜ë¯¸ ì •ë³´ ì¶”ì¶œ
        self.backbone = DINOv3Backbone(model_name)
        feature_dim = self.backbone.model.config.hidden_size
        
        # â–¼â–¼â–¼ [ìˆ˜ì • 1] ë·° ì„ë² ë”©ì„ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë³€ê²½ â–¼â–¼â–¼
        # ìµœëŒ€ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì¹´ë©”ë¼(ë·°) ê°œìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ë² ë”© ë ˆì´ì–´ ìƒì„±
        self.view_embeddings = nn.Embedding(max_views, feature_dim)
        
        # forward ì‹œì ì— ë·° ì´ë¦„/ì‹œë¦¬ì–¼ê³¼ ì¸ë±ìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ë§¤í•‘í•  ë”•ì…”ë„ˆë¦¬
        self.view_to_idx = {} 
        self.next_view_idx = 0

        # 3. CNN ìŠ¤í…œ: ì €ì°¨ì› ê³µê°„ ì •ë³´ ì¶”ì¶œ
        self.cnn_stem = LightCNNStem()
        
        # 4. í“¨ì „ ëª¨ë“ˆ: ëª¨ë“  ë·°ì˜ ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ì „ì—­ ìš”ì•½ ì •ë³´ë¡œ ì••ì¶•
        self.fusion_module = MultiViewFusion(feature_dim=feature_dim)
        
        # 5. í—¤ë“œ (ì˜ˆì¸¡ê¸°)
        self.angle_head = JointAngleHead(input_dim=feature_dim, num_angles=num_angles, num_queries=16)
        self.keypoint_head = UNetViTKeypointHead(input_dim=feature_dim, num_joints=num_joints)
        self.keypoint_enricher = nn.TransformerDecoderLayer(
            d_model=feature_dim, nhead=8, dim_feedforward=feature_dim * 4,
            dropout=0.1, activation='gelu', batch_first=True
        )

    def forward(self, multi_view_images: dict):
        all_dino_features_with_embed = []
        all_cnn_features = {}
        view_keys_ordered = list(multi_view_images.keys())

        # --- Step 1: ê° ë·°ì— ëŒ€í•œ ë³‘ë ¬ íŠ¹ì§• ì¶”ì¶œ ---
        for view_key in view_keys_ordered:
            view_tensor = multi_view_images[view_key]
            dino_features = self.backbone(view_tensor)
            
            if view_key not in self.view_to_idx:
                if self.next_view_idx >= self.view_embeddings.num_embeddings:
                    raise ValueError(f"Exceeded maximum number of views ({self.view_embeddings.num_embeddings}).")
                self.view_to_idx[view_key] = self.next_view_idx
                self.next_view_idx += 1
            
            view_idx = self.view_to_idx[view_key]
            
            embedding = self.view_embeddings(
                torch.tensor([view_idx], device=dino_features.device)
            ).unsqueeze(0)
            all_dino_features_with_embed.append(dino_features + embedding)
            
            all_cnn_features[view_key] = self.cnn_stem(view_tensor)

        # --- Step 2: Multi-view ì •ë³´ ìœµí•© ---
        # Latent Queryë¥¼ í†µí•´ ëª¨ë“  ë·°ì˜ DINO íŠ¹ì§•ì„ 'fused_queries'ë¼ëŠ” ì „ì—­ ì •ë³´ë¡œ ìš”ì•½
        fused_queries = self.fusion_module(all_dino_features_with_embed)
        
        # --- Step 3: ê´€ì ˆ ê°ë„ ì˜ˆì¸¡ ---
        # ìš”ì•½ëœ ì „ì—­ ì •ë³´ë¡œë¶€í„° ì§ì ‘ ê´€ì ˆ ê°ë„ë¥¼ ì˜ˆì¸¡
        predicted_angles = self.angle_head(fused_queries)
        
        # --- Step 4: í‚¤í¬ì¸íŠ¸ íˆíŠ¸ë§µ ì˜ˆì¸¡ ---
        predicted_heatmaps_dict = {}
        for i, view_name in enumerate(view_keys_ordered):
            enriched_tokens = self.keypoint_enricher(
                tgt=all_dino_features_with_embed[i], 
                memory=fused_queries
            )
            heatmap = self.keypoint_head(enriched_tokens, all_cnn_features[view_name])
            predicted_heatmaps_dict[view_name] = heatmap
        
        return predicted_heatmaps_dict, predicted_angles

# ==============================================================================
# Cell 5: í•™ìŠµ/ê²€ì¦ìš© ì‹œê°í™” í•¨ìˆ˜
# ==============================================================================

def visualize_dataset_sample(dataset, mean, std, results_dir, num_samples=1):
    os.makedirs(results_dir, exist_ok=True)
    """ë°ì´í„°ì…‹ì˜ GT ìƒ˜í”Œì„ ì‹œê°í™”í•˜ì—¬ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
    print("\n--- Visualizing Dataset Samples ---")
    # (ì´ì „ Cell 3ì—ì„œ ì‚¬ìš©í–ˆë˜ visualize_final_groups í•¨ìˆ˜ì™€ ê±°ì˜ ë™ì¼í•œ ë¡œì§)
    for i in range(num_samples):
        while True:
            idx = random.randint(0, len(dataset) - 1)
            sample = dataset[idx]
            if sample[0] is not None: break
        
        image_dict, gt_heatmaps_dict, gt_angles = sample
        num_views = len(image_dict)
        fig, axes = plt.subplots(1, num_views, figsize=(6 * num_views, 6))
        if num_views == 1: axes = [axes]

        angle_str = ", ".join([f"{a:.2f}" for a in gt_angles.numpy()])
        fig.suptitle(f"Sample Group {idx} | GT Angles: [{angle_str}]", fontsize=16)

        for j, view_key in enumerate(image_dict.keys()):
            img_tensor = image_dict[view_key]
            img_np = (img_tensor.numpy().transpose(1, 2, 0) * np.array(std)) + np.array(mean)
            img_np = np.clip(img_np, 0, 1)
            H, W, _ = img_np.shape

            gt_heatmaps = gt_heatmaps_dict[view_key]
            heatmap_resized = cv2.resize(torch.sum(gt_heatmaps, dim=0).numpy(), (W, H))
            
            axes[j].imshow(img_np, alpha=0.7)
            axes[j].imshow(heatmap_resized, cmap='jet', alpha=0.3)
            axes[j].set_title(f"View: {view_key} (GT Heatmap)")
            axes[j].axis('off')
            
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        filename = f"gt_sample_{idx}_{int(time.time())}.png"
        plt.savefig(os.path.join(results_dir, filename))
        print(f"  -> Saved GT sample visualization to {os.path.join(results_dir, filename)}")
        plt.close() # ë©”ëª¨ë¦¬ í•´ì œ

def visualize_predictions(model, dataset, device, mean, std, epoch_num, results_dir, num_samples=1):
    """ê²€ì¦ ë°ì´í„°ì…‹ ìƒ˜í”Œì— ëŒ€í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ GTì™€ í•¨ê»˜ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    print(f"\n--- Visualizing Predictions for Epoch {epoch_num} ---")
    os.makedirs(results_dir, exist_ok=True)
    model.eval()
    figures = []
    
    for i in range(num_samples):
        while True:
            idx = random.randint(0, len(dataset) - 1)
            sample = dataset[idx]
            if sample[0] is not None: break

        image_dict, gt_heatmaps_dict, gt_angles = sample

        with torch.no_grad():
            input_batch = {k: v.unsqueeze(0).to(device) for k, v in image_dict.items()}
            pred_heatmaps_dict, pred_angles_batch = model(input_batch)
            pred_angles = pred_angles_batch[0].cpu()

        num_views = len(image_dict)
        fig, axes = plt.subplots(2, num_views, figsize=(6 * num_views, 10))
        if num_views == 1: axes = np.expand_dims(axes, axis=1)

        gt_str = "GT Angles: " + ", ".join([f"{a:.2f}" for a in gt_angles.numpy()])
        pred_str = "Pred Angles: " + ", ".join([f"{a:.2f}" for a in pred_angles.numpy()])
        fig.suptitle(f"Sample {idx} | Epoch {epoch_num}\n{gt_str}\n{pred_str}", fontsize=12)

        for j, view_key in enumerate(image_dict.keys()):
            img_tensor = image_dict[view_key]
            img_np = (img_tensor.numpy().transpose(1, 2, 0) * np.array(std)) + np.array(mean)
            img_np = np.clip(img_np, 0, 1)
            H, W, _ = img_np.shape
            
            # GT Heatmap
            gt_heatmap = torch.sum(gt_heatmaps_dict[view_key], dim=0).numpy()
            axes[0, j].imshow(img_np, alpha=0.7)
            axes[0, j].imshow(cv2.resize(gt_heatmap, (W, H)), cmap='jet', alpha=0.3)
            axes[0, j].set_title(f"View: {view_key} (GT)")
            axes[0, j].axis('off')

            # Predicted Heatmap
            pred_heatmap = torch.sum(pred_heatmaps_dict[view_key][0].cpu(), dim=0).numpy()
            axes[1, j].imshow(img_np, alpha=0.7)
            axes[1, j].imshow(cv2.resize(pred_heatmap, (W, H)), cmap='jet', alpha=0.3)
            axes[1, j].set_title(f"View: {view_key} (Pred)")
            axes[1, j].axis('off')

        plt.tight_layout(rect=[0, 0, 1, 0.92])
        figures.append(fig)

    for i, fig in enumerate(figures):
        filename = f"prediction_epoch_{epoch_num}_sample_{idx}_{i}.png"
        fig.savefig(os.path.join(results_dir, filename))
        print(f"  -> Saved prediction visualization to {os.path.join(results_dir, filename)}")
        # wandb ë¡œê¹…ì„ ìœ„í•´ figure ê°ì²´ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
    return figures

class RandomMasking(object):
    """
    PIL ì´ë¯¸ì§€ì— ë¬´ì‘ìœ„ ì‚¬ê°í˜• ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ëŠ” transform.
    """
    def __init__(self, num_masks=1, mask_size_ratio=(0.1, 0.3), mask_color='random'):
        assert isinstance(num_masks, int) and num_masks > 0
        assert isinstance(mask_size_ratio, tuple) and len(mask_size_ratio) == 2
        self.num_masks = num_masks
        self.mask_size_ratio = mask_size_ratio
        self.mask_color = mask_color

    def __call__(self, img):
        """
        Args:
            img (PIL Image): ì…ë ¥ ì´ë¯¸ì§€.
        Returns:
            PIL Image: ë§ˆìŠ¤í¬ê°€ ì ìš©ëœ ì´ë¯¸ì§€.
        """
        # PIL ì´ë¯¸ì§€ë¥¼ OpenCVê°€ ë‹¤ë£° ìˆ˜ ìˆëŠ” Numpy ë°°ì—´ë¡œ ë³€í™˜ (RGB ìˆœì„œ ìœ ì§€)
        img_np = np.array(img)
        h, w, _ = img_np.shape

        for _ in range(self.num_masks):
            # ë§ˆìŠ¤í¬ í¬ê¸° ê²°ì •
            mask_w = int(w * random.uniform(self.mask_size_ratio[0], self.mask_size_ratio[1]))
            mask_h = int(h * random.uniform(self.mask_size_ratio[0], self.mask_size_ratio[1]))
            
            # ë§ˆìŠ¤í¬ ìœ„ì¹˜ ê²°ì •
            x_start = random.randint(0, w - mask_w)
            y_start = random.randint(0, h - mask_h)
            
            # ë§ˆìŠ¤í¬ ìƒ‰ìƒ ê²°ì •
            if self.mask_color == 'black':
                color = (0, 0, 0)
            elif self.mask_color == 'white':
                color = (255, 255, 255)
            else: # 'random'
                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))
            
            # ì´ë¯¸ì§€ì— ë§ˆìŠ¤í¬ ì ìš©
            img_np[y_start:y_start+mask_h, x_start:x_start+mask_w] = color
        
        # ë‹¤ì‹œ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜
        return Image.fromarray(img_np)

import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

# ==============================================================================
# Cell 6: í•™ìŠµ ë° ê²€ì¦ ë£¨í”„ ì •ì˜
# ==============================================================================

def train_one_epoch(model, loader, optimizers, criteria, device, loss_weight_kpt, epoch_num):
    model.train()
    total_loss_kpt, total_loss_ang = 0.0, 0.0
    optimizer_kpt, optimizer_ang = optimizers['kpt'], optimizers['ang']
    crit_kpt, crit_ang = criteria['kpt'], criteria['ang']

    loop = tqdm(loader, desc=f"Epoch {epoch_num} [Train]")
    for batch in loop:
        image_dict, gt_heatmaps_dict, gt_angles = batch

        # ---- (A) ë­í¬ ê°„ 'ìœ íš¨ ë°°ì¹˜' ì—¬ë¶€ ë™ê¸°í™” ----
        has_data_local = int(image_dict is not None)
        has_data_all = torch.tensor(has_data_local, device=device)
        import torch.distributed as dist
        dist.all_reduce(has_data_all, op=dist.ReduceOp.SUM)
        has_any_rank_data = int(has_data_all.item())

        if not has_any_rank_data:
            # ëª¨ë“  ë­í¬ê°€ ë¹ˆ ë°°ì¹˜ì´ë©´ ì „ì²´ ìŠ¤í‚µ
            continue

        if image_dict is None:
            # ì´ ë­í¬ë§Œ ë¹ˆ ë°°ì¹˜ì¸ ê²½ìš°: ë”ë¯¸ 0-loss backwardë¡œ í†µì‹  íƒ€ì´ë° ë§ì¶¤
            optimizer_kpt.zero_grad(set_to_none=True)
            optimizer_ang.zero_grad(set_to_none=True)
            # ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ 'ì •ì˜ëœ(=0) grad'ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ í•©ê³„ë¥¼ 0ë°° í•¨
            dummy = None
            for p in model.parameters():
                if p.requires_grad:
                    dummy = (p.sum() if dummy is None else dummy + p.sum())
            if dummy is None:
                # ì´ë¡ ìƒ ë°œìƒX: í•™ìŠµ íŒŒë¼ë¯¸í„°ê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°
                dummy = torch.zeros((), device=device, requires_grad=True)
            (dummy * 0.0).backward()
            optimizer_kpt.step()
            optimizer_ang.step()
            loop.set_postfix(loss_kpt='skip', loss_ang='skip')
            continue

        # ---- (B) ì •ìƒ ê²½ë¡œ: ë‹¨ì¼ backward ----
        images_gpu   = {k: v.to(device) for k, v in image_dict.items()}
        heatmaps_gpu = {k: v.to(device) for k, v in gt_heatmaps_dict.items()}
        angles_gpu   = gt_angles.to(device)

        pred_heatmaps_dict, pred_angles = model(images_gpu)

        loss_ang = crit_ang(pred_angles, angles_gpu)

        real_view_keys = list(pred_heatmaps_dict.keys())
        if not real_view_keys:
            # ê·¹íˆ ë“œë¬¼ê²Œ í‚¤ ì—†ìŒ â†’ ë”ë¯¸ backwardë¡œ ì •ë ¬
            optimizer_kpt.zero_grad(set_to_none=True)
            optimizer_ang.zero_grad(set_to_none=True)
            dummy = None
            for p in model.parameters():
                if p.requires_grad:
                    dummy = (p.sum() if dummy is None else dummy + p.sum())
            (dummy * 0.0).backward()
            optimizer_kpt.step()
            optimizer_ang.step()
            loop.set_postfix(loss_kpt='skip2', loss_ang='skip2')
            continue

        loss_kpt_views = [crit_kpt(pred_heatmaps_dict[k], heatmaps_gpu[k]) for k in real_view_keys]
        loss_kpt = torch.stack(loss_kpt_views).mean() * loss_weight_kpt

        total_loss = loss_kpt + loss_ang

        optimizer_kpt.zero_grad(set_to_none=True)
        optimizer_ang.zero_grad(set_to_none=True)
        total_loss.backward()   # â† í•œ ë²ˆë§Œ!
        optimizer_kpt.step()
        optimizer_ang.step()

        total_loss_kpt += loss_kpt.item()
        total_loss_ang += loss_ang.item()
        loop.set_postfix(loss_kpt=loss_kpt.item(), loss_ang=loss_ang.item())

    return total_loss_kpt / len(loader), total_loss_ang / len(loader)



def validate(model, loader, criteria, device, loss_weight_kpt, epoch_num):
    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    total_val_loss = 0.0
    crit_kpt, crit_ang = criteria['kpt'], criteria['ang']
    
    with torch.no_grad():
        for batch in tqdm(loader, desc=f"Epoch {epoch_num} [Validate]", leave=False):
            image_dict, gt_heatmaps_dict, gt_angles = batch
            if image_dict is None: continue

            images_gpu = {k: v.to(device) for k, v in image_dict.items()}
            heatmaps_gpu = {k: v.to(device) for k, v in gt_heatmaps_dict.items()}
            angles_gpu = gt_angles.to(device)
            
            pred_heatmaps_dict, pred_angles = model(images_gpu)
            
            loss_ang = crit_ang(pred_angles, angles_gpu)
            
            real_view_keys = [k for k in pred_heatmaps_dict if not k.startswith('dummy')]
            if not real_view_keys: continue
                
            loss_kpt_views = [crit_kpt(pred_heatmaps_dict[k], heatmaps_gpu[k]) for k in real_view_keys]
            loss_kpt = (torch.stack(loss_kpt_views).mean()) * loss_weight_kpt
            
            total_loss = loss_kpt + loss_ang
            total_val_loss += total_loss.item()
            
    return total_val_loss / len(loader)

# ==============================================================================
# Cell 7: í•™ìŠµ í™˜ê²½ ì„¤ì • (Setup) í•¨ìˆ˜
# ==============================================================================
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from transformers import AutoImageProcessor

# â–¼â–¼â–¼ DDP í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ â–¼â–¼â–¼
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

# ==============================================================================
# Setup / Teardown í•¨ìˆ˜
# ==============================================================================
def setup_ddp():
    dist.init_process_group(backend="nccl")
    rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(rank)
    return rank

def cleanup_ddp():
    dist.destroy_process_group()

def setup(hyperparameters, dataset_groups, rank, world_size):
    print(f"--- [Rank {rank}] Setting up environment ---")
    device = torch.device(f'cuda:{rank}')
    
    processor = AutoImageProcessor.from_pretrained(hyperparameters['model_name'])
    mean, std = processor.image_mean, processor.image_std
    resize_size, crop_size = 512, 512
    
    train_transform = transforms.Compose([
        transforms.Resize(resize_size), transforms.CenterCrop(crop_size),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)
    ])
    val_transform = transforms.Compose([
        transforms.Resize(resize_size), transforms.CenterCrop(crop_size),
        transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)
    ])
    
    torch.manual_seed(42)
    indices = torch.randperm(len(dataset_groups)).tolist()
    train_size = int(len(dataset_groups) * (1 - hyperparameters['val_split']))
    train_groups = [dataset_groups[i] for i in indices[:train_size]]
    val_groups = [dataset_groups[i] for i in indices[train_size:]]
    
    train_dataset = RobotPoseDataset(groups=train_groups, transform=train_transform)
    val_dataset = RobotPoseDataset(groups=val_groups, transform=val_transform)
    
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)
    
    def collate_fn(batch):
        batch = [b for b in batch if b[0] is not None]
        if not batch: return None, None, None
        image_dicts, heatmap_dicts, angles_list = zip(*batch)
        all_keys = set().union(*[d.keys() for d in image_dicts])
        sample_img = list(image_dicts[0].values())[0]
        dummy_img = torch.zeros_like(sample_img)
        sample_hmap = list(heatmap_dicts[0].values())[0]
        dummy_hmap = torch.zeros_like(sample_hmap)
        standardized_images, standardized_heatmaps = [], []
        for i in range(len(batch)):
            new_img_dict = {key: image_dicts[i].get(key, dummy_img) for key in all_keys}
            new_hmap_dict = {key: heatmap_dicts[i].get(key, dummy_hmap) for key in all_keys}
            standardized_images.append(new_img_dict)
            standardized_heatmaps.append(new_hmap_dict)
        images = torch.utils.data.dataloader.default_collate(standardized_images)
        heatmaps = torch.utils.data.dataloader.default_collate(standardized_heatmaps)
        angles = torch.stack(angles_list)
        return images, heatmaps, angles

    train_loader = DataLoader(train_dataset, batch_size=hyperparameters['batch_size'], num_workers=8, collate_fn=collate_fn, pin_memory=True, sampler=train_sampler)
    val_loader = DataLoader(val_dataset, batch_size=hyperparameters['batch_size'], num_workers=8, collate_fn=collate_fn, pin_memory=True, sampler=val_sampler)
    
    model = DINOv3PoseEstimator(hyperparameters['model_name']).to(device)
    model = DDP(model, device_ids=[rank], find_unused_parameters=True)
    
    criteria = {'kpt': nn.MSELoss(), 'ang': nn.SmoothL1Loss(beta=1.0)}
    
    m = model.module
    params_kpt = list(m.cnn_stem.parameters()) + list(m.view_embeddings.parameters()) + list(m.fusion_module.parameters()) + list(m.keypoint_enricher.parameters()) + list(m.keypoint_head.parameters())
    params_ang = list(m.angle_head.parameters())
    
    optimizers = { 'kpt': optim.AdamW(params_kpt, lr=hyperparameters['lr_kpt']), 'ang': optim.AdamW(params_ang, lr=hyperparameters['lr_ang']) }
    schedulers = { 'kpt': CosineAnnealingLR(optimizers['kpt'], T_max=hyperparameters['num_epochs']), 'ang': CosineAnnealingLR(optimizers['ang'], T_max=hyperparameters['num_epochs']) }
    
    if rank == 0: print(f"Dataset split: {len(train_dataset)} train, {len(val_dataset)} val.")
    return model, train_loader, val_loader, criteria, optimizers, schedulers, device, mean, std, train_sampler

# ==============================================================================
# Cell 8: ë©”ì¸ ì‹¤í–‰ë¶€
# ==============================================================================
import time

def main():
    rank = setup_ddp()
    world_size = dist.get_world_size()

    # --- ğŸ–¥ï¸ GPU ì„¤ì • í™•ì¸ (ë‚´ë¶€ì—ì„œ CUDA_VISIBLE_DEVICES ì„¤ì • X) ---
    os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
    if torch.cuda.is_available():
        if rank == 0:
            print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {torch.cuda.device_count()}ê°œ")
    else:
        if rank == 0:
            print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    # --- ğŸ“„ CSV ë¡œë“œ (rank==0ì—ì„œë§Œ ë””ìŠ¤í¬ I/O) ---
    TOTAL_CSV_PATH = '../dataset/franka_research3/fr3_matched_joint_angle.csv'
    if rank == 0:
        print(f"\nLoading data from {TOTAL_CSV_PATH}...")
        total_csv = pd.read_csv(TOTAL_CSV_PATH)
        total_csv.sort_values('robot_timestamp', inplace=True, ignore_index=True)
        print("âœ… CSV file loaded and sorted successfully.")
    else:
        total_csv = None  # placeholder

    # --- ëª¨ë“  ë­í¬ì— CSV ë¸Œë¡œë“œìºìŠ¤íŠ¸ ---
    obj_list = [total_csv]
    dist.broadcast_object_list(obj_list, src=0)
    total_csv = obj_list[0]

    # --- TIME_TOLERANCE ê·¸ë¦¬ë“œ ì„œì¹˜ (ì¶œë ¥ì€ rank==0ë§Œ) ---
    tolerance_candidates = np.round(np.arange(0.05, 0.101, 0.01), 2)
    best_tolerance_recommendation = 0
    max_full_groups = 0

    if rank == 0:
        print(f"\nStarting Grid Search for TIME_TOLERANCE in range: {list(tolerance_candidates)}")
    for tolerance in tolerance_candidates:
        temp_groups = perform_grouping(total_csv, tolerance, MAX_VIEWS_PER_GROUP)
        view_counts = [len(g['views']) for g in temp_groups]
        distribution = pd.Series(view_counts).value_counts().sort_index(ascending=False)

        if rank == 0:
            print("-" * 50)
            print(f"Testing Tolerance: {tolerance:.2f} seconds...")
            print(f"  -> Total groups created: {len(temp_groups)}")
            print("  -> View count distribution:")
            print(distribution.to_string())

        current_full_groups = distribution.get(8, 0)
        if current_full_groups > max_full_groups:
            max_full_groups = current_full_groups
            best_tolerance_recommendation = tolerance

    if rank == 0:
        print("-" * 50)
        print(f"\nğŸ† Grid Search Recommendation: TIME_TOLERANCE = {best_tolerance_recommendation} (produced {max_full_groups} full groups)")

    # --- ìµœì¢… tolerance ì ìš© ë° ê·¸ë£¹ ìƒì„± ---
    final_tolerance = 0.07
    if rank == 0:
        print(f"\nFinal TIME_TOLERANCE set to: {final_tolerance}")
    dataset_groups = perform_grouping(total_csv, final_tolerance, MAX_VIEWS_PER_GROUP)
    if rank == 0:
        print(f"Total {len(dataset_groups)} groups created before filtering.")

    # --- 1ë·° ê·¸ë£¹ ì œê±° ---
    groups_before_filtering = len(dataset_groups)
    dataset_groups = [group for group in dataset_groups if len(group['views']) > 1]
    if rank == 0:
        print(f"â„¹ï¸ Removed {groups_before_filtering - len(dataset_groups)} groups with only 1 view.")
        print(f"\nâœ… Final Total Groups: {len(dataset_groups)}")
        total_images_in_groups = sum(len(g['views']) for g in dataset_groups)
        print(f"âœ… Final Total Images to be used: {total_images_in_groups}")
        if dataset_groups:
            view_counts = [len(g['views']) for g in dataset_groups]
            print(f"\n--- Final View count distribution ---")
            print(pd.Series(view_counts).value_counts().sort_index(ascending=False))

    # --- í•˜ì´í¼íŒŒë¼ë¯¸í„° & ê²½ë¡œ ---
    hyperparameters = {
        'model_name': MODEL_NAME, 'batch_size': 18, 'num_epochs': 100, 'val_split': 0.1,
        'loss_weight_kpt': 100.0, 'lr_kpt': 1e-4, 'lr_ang': 1e-4,
    }
    RESULTS_DIR = "results_ddp"
    CHECKPOINT_PATH = 'multiview_checkpoint_ddp.pth'
    BEST_MODEL_PATH = 'best_multiview_model_ddp.pth'

    if rank == 0:
        os.makedirs(RESULTS_DIR, exist_ok=True)
        print("--- Data Preparation ---")

    # --- dataset_groups ê°ì²´ ë¸Œë¡œë“œìºìŠ¤íŠ¸ (íŒŒì¼ ëŒ€ì‹  broadcast_object_list ì‚¬ìš©) ---
    obj_list = [dataset_groups]
    dist.broadcast_object_list(obj_list, src=0)
    dataset_groups = obj_list[0]

    # --- DINOv3 Processor ë¡œë“œ (ëª¨ë“  ë­í¬ ë™ì¼) ---
    if rank == 0:
        print("Loading DINOv3 Processor for transformation config...")
    processor = AutoImageProcessor.from_pretrained(MODEL_NAME)
    dino_mean = processor.image_mean
    dino_std = processor.image_std
    try:
        crop_size = processor.crop_size['height']
        resize_size = processor.size['shortest_edge']
    except (TypeError, KeyError):
        if rank == 0:
            print(f"Resized the image to 224x224")
        resize_size = crop_size = 224

    vis_transform = transforms.Compose([
        transforms.Resize(resize_size),
        transforms.CenterCrop(crop_size),
        transforms.ToTensor(),
        transforms.Normalize(mean=dino_mean, std=dino_std)
    ])

    # --- ì‹œê°í™”ëŠ” rank==0ì—ì„œë§Œ (plt.show ì‚¬ìš©í•˜ì§€ ë§ê³  ì €ì¥ë§Œ) ---
    if rank == 0:
        visualize_samples_by_group_size(dataset_groups, transform=vis_transform, mean=dino_mean, std=dino_std)

    dist.barrier()  # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ë™ê¸°í™”

    # --- í•™ìŠµ ì„¸íŒ… ---
    model, train_loader, val_loader, criteria, optimizers, schedulers, device, mean, std, train_sampler = setup(
        hyperparameters, dataset_groups, rank, world_size
    )

    # visualize_* í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  ì „ì—­ ê²°ê³¼ ê²½ë¡œ ì£¼ì… (í•´ë‹¹ í•¨ìˆ˜ê°€ results_dirë¥¼ ì „ì—­ ì°¸ì¡°í•  ë•Œ ëŒ€ë¹„)
    global results_dir
    results_dir = RESULTS_DIR

    # --- wandb (rank==0 í•œì •) ---
    if rank == 0:
        run = wandb.init(project="multiview-ddp-final", config=hyperparameters,
                         name=f"run_ddp_{time.strftime('%Y%m%d_%H%M%S')}")
        wandb.watch(model, log="all", log_freq=100)
    else:
        run = None

    start_epoch, best_val_loss = 0, float('inf')

    # --- ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ---
    if os.path.exists(CHECKPOINT_PATH):
        checkpoint = torch.load(CHECKPOINT_PATH, map_location=lambda storage, loc: storage.cuda(rank))
        model.module.load_state_dict(checkpoint['model_state_dict'])
        # (ì˜µí‹°ë§ˆì´ì €/ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë³µì› í•„ìš” ì‹œ ì—¬ê¸°ì— ì¶”ê°€)
        if rank == 0:
            print(f"âœ… Resuming training from checkpoint.")

    # --- í•™ìŠµ ë£¨í”„ ---
    if rank == 0:
        print("\n--- Starting Training ---")
    for epoch in range(start_epoch, hyperparameters['num_epochs']):
        train_sampler.set_epoch(epoch)

        train_loss_kpt, train_loss_ang = train_one_epoch(
            model, train_loader, optimizers, criteria, device, hyperparameters['loss_weight_kpt'], epoch + 1
        )
        val_loss = validate(
            model, val_loader, criteria, device, hyperparameters['loss_weight_kpt'], epoch + 1
        )
        schedulers['kpt'].step(); schedulers['ang'].step()

        if rank == 0:
            current_lr_kpt = optimizers['kpt'].param_groups[0]['lr']
            wandb.log({
                "epoch": epoch + 1,
                "train_loss_kpt": train_loss_kpt,
                "train_loss_ang": train_loss_ang,
                "avg_val_loss": val_loss,
                "lr_kpt": current_lr_kpt
            })
            print(f"Epoch {epoch+1} -> Val Loss: {val_loss:.6f} | LR_kpt: {current_lr_kpt:.6f}")

            # DataParallel ë¶„ê¸° ë¶ˆí•„ìš” (DDP ì‚¬ìš© ì¤‘)ì§€ë§Œ ì•ˆì „í•˜ê²Œ ìœ ì§€
            state_to_save = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                print(f"ğŸ‰ New best model saved with validation loss: {best_val_loss:.6f}")
                torch.save(state_to_save, BEST_MODEL_PATH)

                # ì£¼ì˜: visualize_predictions ì‹œê·¸ë‹ˆì²˜ê°€ (model, dataset, device, mean, std, epoch_num, num_samples=1) ì¸ ê²½ìš°
                # ì•„ë˜ í˜¸ì¶œì—ì„œ RESULTS_DIR ì¸ìë¥¼ ì œê±°í•˜ì„¸ìš”. (í˜„ì¬ ì½”ë“œì—” RESULTS_DIRë¥¼ ì „ì—­ results_dirë¡œ ì£¼ì…)
                figs = visualize_predictions(model, val_loader.dataset, device, mean, std, epoch + 1, RESULTS_DIR, 1)
                wandb.log({"validation_predictions": [wandb.Image(fig) for fig in figs]})
                for fig in figs:
                    plt.close(fig)

            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': state_to_save,
                'optimizer_kpt_state_dict': optimizers['kpt'].state_dict(),
                'optimizer_ang_state_dict': optimizers['ang'].state_dict(),
                'scheduler_kpt_state_dict': schedulers['kpt'].state_dict(),
                'scheduler_ang_state_dict': schedulers['ang'].state_dict(),
                'best_val_loss': best_val_loss,
            }
            torch.save(checkpoint, CHECKPOINT_PATH)

    cleanup_ddp()

    if rank == 0:
        print("\n--- Training Finished ---")
        if run is not None:
            run.finish()

if __name__ == '__main__':
    main()
